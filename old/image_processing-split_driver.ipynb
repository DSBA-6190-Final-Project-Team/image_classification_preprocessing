{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "## Project Folder Structure\n",
    "The input data for this project is too large to store with the github repo. Therefore, the files need to be added manually. The raw image files can be found here:\n",
    "\n",
    "[https://www.kaggle.com/c/state-farm-distracted-driver-detection/data](https://www.kaggle.com/c/state-farm-distracted-driver-detection/data) (Accessed on 3/21/2020)\n",
    "\n",
    "You can download the raw input files as a zip file. It will contain the following:\n",
    "\n",
    "\n",
    "* driver_imgs_list.csv (This file in maintained in the repo)\n",
    "* sample_submission.csv (This file is maintained in the repo)\n",
    "* imgs folder, with test and train subfolders\n",
    "\n",
    "This **imgs** folder is the part of the input which is not maintained in the repo. Therefore, for this notebook to function the data in the **imgs** \n",
    "\n",
    "--Working\n",
    "\n",
    "----imgs\n",
    "\n",
    "------raw\n",
    "  \n",
    "--------train\n",
    "\n",
    "--------test\n",
    "\n",
    "The train and test folders come directly from the kaggle dataset. Subsequent folders will be generated by this notebook.\n",
    "\n",
    "## Scope\n",
    "This notebook is to prepare the images in the distracted driver dataset locally, for upload to S3.\n",
    "\n",
    "This notebook will create two versions of the data. The first will be a LST mapping file, with the image files in JPEG format. The second will be a RECORDIO format.\n",
    "\n",
    "A Complete dataset and Sample dataset will be created and uploaded.\n",
    "\n",
    "## Method\n",
    "\n",
    "This notebook splits the training and validation set manually by driver, not by random selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone MXNET to get im2rec tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/apache/incubator-mxnet.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: checksumdir in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (1.1.7)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (4.2.0.32)\n",
      "Requirement already satisfied: mxnet in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from opencv-python) (1.15.0)\n",
      "Requirement already satisfied: requests<2.19.0,>=2.18.4 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from mxnet) (2.18.4)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2019.11.28)\n"
     ]
    }
   ],
   "source": [
    "!pip install checksumdir\n",
    "!pip install opencv-python mxnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1z9yTpGbACbX"
   },
   "source": [
    "## Library / Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhXJ8brV_4Qb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random \n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "\n",
    "import hashlib\n",
    "from checksumdir import dirhash\n",
    "\n",
    "from filecmp import dircmp\n",
    "\n",
    "#Settings\n",
    "seed = 5590"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "406FN849AG3C"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RpUao_lNAE88"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/DSBA-6190-Final-Project-Team/DSBA-6190_Final-Project/master/wine_predict/data/driver_imgs_list.csv\"\n",
    "path_file = 'data/driver_imgs_list.csv'\n",
    "\n",
    "df_driver_index = pd.read_csv(path_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xr04mwBdAwNY"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1W_1IwhLA7vT",
    "outputId": "6c263b25-d00d-46cc-c0cc-35a5f5e9f47c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22424, 3)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_driver_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "n3glHqjBA3wI",
    "outputId": "d57b86d9-fe7b-4640-f241-3cba70bfab9e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_44733.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_72999.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_25094.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_69092.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_92629.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject classname            img\n",
       "0    p002        c0  img_44733.jpg\n",
       "1    p002        c0  img_72999.jpg\n",
       "2    p002        c0  img_25094.jpg\n",
       "3    p002        c0  img_69092.jpg\n",
       "4    p002        c0  img_92629.jpg"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_driver_index.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R-sfjF4eAyMI",
    "outputId": "2a8fafe6-343c-4cc7-8b1c-5f64856d2518"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'classname', 'img'], dtype='object')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_driver_index.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2w7VZnGBz8G"
   },
   "source": [
    "The following lists each unique driver along with the number of different classname and images are associated with each. Note the number of classnames is not unique, so the images an classnames have equal frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 886
    },
    "colab_type": "code",
    "id": "_vqngHEZBEVL",
    "outputId": "f391960d-5245-42b3-ba56-0cde324c5a5b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p002</th>\n",
       "      <td>725</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p012</th>\n",
       "      <td>823</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p014</th>\n",
       "      <td>876</td>\n",
       "      <td>876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p015</th>\n",
       "      <td>875</td>\n",
       "      <td>875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p016</th>\n",
       "      <td>1078</td>\n",
       "      <td>1078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p021</th>\n",
       "      <td>1237</td>\n",
       "      <td>1237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p022</th>\n",
       "      <td>1233</td>\n",
       "      <td>1233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p024</th>\n",
       "      <td>1226</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p026</th>\n",
       "      <td>1196</td>\n",
       "      <td>1196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p035</th>\n",
       "      <td>848</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p039</th>\n",
       "      <td>651</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p041</th>\n",
       "      <td>605</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p042</th>\n",
       "      <td>591</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p045</th>\n",
       "      <td>724</td>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p047</th>\n",
       "      <td>835</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p049</th>\n",
       "      <td>1011</td>\n",
       "      <td>1011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p050</th>\n",
       "      <td>790</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p051</th>\n",
       "      <td>920</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p052</th>\n",
       "      <td>740</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p056</th>\n",
       "      <td>794</td>\n",
       "      <td>794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p061</th>\n",
       "      <td>809</td>\n",
       "      <td>809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p064</th>\n",
       "      <td>820</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p066</th>\n",
       "      <td>1034</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p072</th>\n",
       "      <td>346</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p075</th>\n",
       "      <td>814</td>\n",
       "      <td>814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p081</th>\n",
       "      <td>823</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         classname   img\n",
       "subject                 \n",
       "p002           725   725\n",
       "p012           823   823\n",
       "p014           876   876\n",
       "p015           875   875\n",
       "p016          1078  1078\n",
       "p021          1237  1237\n",
       "p022          1233  1233\n",
       "p024          1226  1226\n",
       "p026          1196  1196\n",
       "p035           848   848\n",
       "p039           651   651\n",
       "p041           605   605\n",
       "p042           591   591\n",
       "p045           724   724\n",
       "p047           835   835\n",
       "p049          1011  1011\n",
       "p050           790   790\n",
       "p051           920   920\n",
       "p052           740   740\n",
       "p056           794   794\n",
       "p061           809   809\n",
       "p064           820   820\n",
       "p066          1034  1034\n",
       "p072           346   346\n",
       "p075           814   814\n",
       "p081           823   823"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drivers_gb = df_driver_index.groupby(['subject'])\n",
    "drivers_gb.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0KTpe42Cqi6"
   },
   "source": [
    "We will set the training/validation split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SNDV8uWICp9y"
   },
   "outputs": [],
   "source": [
    "train_val_split = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6gBnk_kDXtt"
   },
   "source": [
    "To split the data into training and validation sets, we'll need a unique index of drivers. We don't need the frequency counts.\n",
    "\n",
    "We will create a shuffled list of unique drivers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "5Lps82L_Dn9-",
    "outputId": "f0aff480-dc8e-471a-d45f-ee9fd2f8d09e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p049', 'p022', 'p012', 'p075', 'p045', 'p072', 'p026', 'p042', 'p015', 'p039', 'p047', 'p081', 'p052', 'p014', 'p021', 'p035', 'p061', 'p064', 'p066', 'p056', 'p002', 'p041', 'p024', 'p016', 'p051', 'p050']\n"
     ]
    }
   ],
   "source": [
    "drivers_unique = drivers_gb.groups.keys()\n",
    "drivers_unique = list(drivers_unique)\n",
    "\n",
    "random.Random(seed).shuffle(drivers_unique)\n",
    "print(drivers_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YOClOK2lEZsD"
   },
   "source": [
    "We'll set the list of drivers in the train set and the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rzmgBKnHEfmx"
   },
   "outputs": [],
   "source": [
    "num_drivers_val = round(len(drivers_unique)*train_val_split)\n",
    "#print(num_drivers_val)\n",
    "num_drivers_train = len(drivers_unique) - num_drivers_val\n",
    "#print(num_drivers_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "keIIvYaDFeGv",
    "outputId": "a0445d0d-d0e2-4780-a6ab-8ae3abce0bca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p049', 'p022', 'p012', 'p075', 'p045']\n"
     ]
    }
   ],
   "source": [
    "drivers_val = drivers_unique[:num_drivers_val]\n",
    "print(drivers_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aA1httPIFte_",
    "outputId": "e23ed327-f12f-408b-bf00-8cfeb1d49f4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p072', 'p026', 'p042', 'p015', 'p039', 'p047', 'p081', 'p052', 'p014', 'p021', 'p035', 'p061', 'p064', 'p066', 'p056', 'p002', 'p041', 'p024', 'p016', 'p051', 'p050']\n"
     ]
    }
   ],
   "source": [
    "drivers_train = drivers_unique[-num_drivers_train:]\n",
    "print(drivers_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hNA4cd_cPquX"
   },
   "source": [
    "# Training and Validation Image Lists\n",
    "We'll now create two lists, one list of every image file name associated with the trainging set, another list associated with the validation set. \n",
    "\n",
    "We will use these lists to filter the overall lst mapping file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f8a4uhBKP-uQ"
   },
   "outputs": [],
   "source": [
    "df_images_val = df_driver_index[df_driver_index['subject'].isin(drivers_val)]\n",
    "df_images_train = df_driver_index[~df_driver_index['subject'].isin(drivers_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "vHXC2dsXQ2Po",
    "outputId": "d2f863d9-7bdb-493f-d31e-148314e19378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17819, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_44733.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_72999.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_25094.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_69092.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_92629.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject classname            img\n",
       "0    p002        c0  img_44733.jpg\n",
       "1    p002        c0  img_72999.jpg\n",
       "2    p002        c0  img_25094.jpg\n",
       "3    p002        c0  img_69092.jpg\n",
       "4    p002        c0  img_92629.jpg"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_images_train.shape)\n",
    "df_images_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "ex0G4NQCRn0j",
    "outputId": "cc7473cf-2acc-41b2-aa75-507a59349d7c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4605, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_10206.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_27079.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_50749.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_97089.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_37741.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject classname            img\n",
       "725    p012        c0  img_10206.jpg\n",
       "726    p012        c0  img_27079.jpg\n",
       "727    p012        c0  img_50749.jpg\n",
       "728    p012        c0  img_97089.jpg\n",
       "729    p012        c0  img_37741.jpg"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_images_val.shape)\n",
    "df_images_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move Images\n",
    "We need to create two folders images, one for training, and one for validation. We will leave the complete folder of images alone, in case we need to go back and extract information from that folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Copy\n",
    "The following function takes the training or validaiton dataframe and iterates through each row. It then copies each file from the overall data to the respective folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_imgs_driver(status, df):\n",
    "    prefix_subfolder = \"imgs\"\n",
    "    prefix_src = \"raw/train\"\n",
    "    prefix_dst = \"train-split_driver\"\n",
    "    #prefix_status = os.path.join(\"imgs\",folder_name)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        path_src = os.path.join(prefix_subfolder,\n",
    "                                prefix_src,\n",
    "                                row['classname'], \n",
    "                                row['img']).replace('\\\\','/')\n",
    "             \n",
    "       \n",
    "        path_dst = os.path.join(prefix_subfolder,\n",
    "                                prefix_dst,\n",
    "                                status,\n",
    "                                row['classname'],\n",
    "                                row['img']).replace('\\\\','/')\n",
    "        \n",
    "        #print(\"Source File:\\t\\t{}\".format(path_src))\n",
    "        #print(\"Destination File:\\t{}\".format(path_dst))\n",
    "    \n",
    "        \n",
    "        if not os.path.exists(path_dst):\n",
    "            # Verify Directory Exists. Create if Not \n",
    "            os.makedirs(os.path.dirname(path_dst), exist_ok=True)\n",
    "            \n",
    "            shutil.copy(path_src, path_dst)\n",
    "            #print(\"Loop Start\")\n",
    "            #print(\"Source File:\\t\\t{}\".format(path_src))\n",
    "            #print(\"Destination File:\\t{}\".format(path_dst))\n",
    "            #print()\n",
    "        #else:\n",
    "            #print(\"File Exists. No Copy Made.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a dictionary containing the status and associated dataframes. This will be looped over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_image_df = {\n",
    "    \"train\": df_images_train,\n",
    "    \"validation\" : df_images_val,\n",
    "    \"dummy\": df_images_val\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_image_df:\n",
    "    copy_imgs_driver(key, dict_image_df[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Folder Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now verify the size of the resulting folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calculates the number of files in a directory, recursively\n",
    "def num_files(dir_scan):\n",
    "    cpt = sum([len(files) for r, d, files in os.walk(dir_scan)])\n",
    "    print(\"Directory Analyzed:\\t{}\".format(dir_scan))\n",
    "    print(\"\\t\\t\\tThere are {} image files in the directory.\".format(cpt))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Analyzed:\tD:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\imgs\\train-split_driver\\train\n",
      "\t\t\tThere are 17819 image files in the directory.\n",
      "\n",
      "Directory Analyzed:\tD:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\imgs\\train-split_driver\\validation\n",
      "\t\t\tThere are 4605 image files in the directory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_current = \"D:\\\\Notebooks\\\\dsba_6190\\\\team_project\\\\image_classification_preprocessing\"\n",
    "prefix_split = \"imgs\\\\train-split_driver\"\n",
    "prefix_train = \"train\"\n",
    "prefix_val = \"validation\"\n",
    "\n",
    "# Training\n",
    "dir_train = os.path.join(path_current, prefix_split, prefix_train)\n",
    "num_files(dir_train)\n",
    "\n",
    "# Validation\n",
    "dir_validation =  os.path.join(path_current, prefix_split, prefix_val)\n",
    "num_files(dir_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Flow\n",
    "\n",
    "Some of the downstream processing steps are computationally heavy. Therefore, I'm going to put in some guardrails in order to avoid running these code chunks, without manually commenting them out once run. \n",
    "\n",
    "To check if any of the input files have changed, which would then require reprocessing the images, we will use Hash checksum values to document the state of the inputs. We will check against this state to determine if we need to reprocess the data. \n",
    "\n",
    "To do this, we will have to convert the processing steps into callable functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## im2rec Function Calls\n",
    "The following functions use the im2rec.py tool to process the image inputs.\n",
    "\n",
    "[https://mxnet.apache.org/api/faq/recordio](https://mxnet.apache.org/api/faq/recordio) (Accessed on 3/20/2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LST File Generation\n",
    "This function performs the first step, creating LST mapping files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2rec_lst(status, status_drop_train, dataset):\n",
    "    lst_file_name = dataset\n",
    "    lst_file_path = os.path.join(\"model_input_files\", status_drop_train, dataset)\n",
    "    img_loc =  os.path.join(\"imgs\", status, dataset)\n",
    "    \n",
    "    %run incubator-mxnet/tools/im2rec.py --list --recursive {lst_file_path} {img_loc} \n",
    "    \n",
    "    return img_loc, lst_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recordio Conversion\n",
    "This function converts the images into a RECORDIO binary file format. In addition to converting images from raw image files to RECORDIO, we need to resize the images. In the Amazon Sagemaker Image Classification Hyperparameters documentation ([here](https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html)) there is a parameter called **image_shape**. This parameter requires a string with three numbers, comma seperated (i.e. \"1,2,3\").\n",
    "\n",
    "The first value is **num_channels**. This is the number of channels our input images have. They are color images, so they have three channels (Red, Green, and Blue aka RGB). The second and third values are the heigth and width of the images, in pixels. Technically, the algorithm can accomadate images of any size, but if the images are too large, there may be memory constraints. It indicates typical dimensions are 244 x 244. Our images come are 640 x 480. By area this is 5x bigger. So, we need to resize the images.\n",
    "\n",
    "The tool **im2rec.py** is capable of resizing images during the RECORDIO conversion. All we need to to is add the argument \"**--resize #**\" to the command line entry. The number in the command line argument is what the tool will resize the ***shortest edge*** of the input image. The larger edge will be resized to maintain the same relative dimensions. \n",
    "\n",
    "Therefore, we are going to resize the input images so that the resulting area is approximately equal to the area of a 244 x 244 image. The dimensions we are going to use is 210 x 280. This maintains the relative size of the input images, while have an area 98.7% that of the default image size. So, in the command line argument, the call will be **--resize 210**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2rec_rec(img_loc, lst_file_path):\n",
    "    \n",
    "    %run incubator-mxnet/tools/im2rec.py  --resize 210 {lst_file_path} {img_loc} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Hash Checksum\n",
    "The following function generates the Hash of a directory (training or validation images) and writes the Hash in a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_hash(status, file_path, file_dir):\n",
    "\n",
    "    # Generate Hash\n",
    "    dir_hash = dirhash(file_dir, 'sha256')\n",
    "    \n",
    "    # Write to CSV\n",
    "    dict_dir_hash = {'hash': [dir_hash]}\n",
    "    df_dir_hash = pd.DataFrame(dict_dir_hash)\n",
    "    df_dir_hash.to_csv(file_path, index=False)\n",
    "    print(\"Hash for {} images successfully written to CSV.\".format(status))\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Processing and Writing Hash\n",
    "Whenever we process the input data we need to write a new hash, and vice versa. Therefore, for simplicity in the final code, we will lump these actions together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2rec_and_write(status, status_drop_train, dataset, file_name, file_path, file_dir):\n",
    "        print(\"Generating LST File: {} status - {} dataset\".format(status, dataset))\n",
    "        \n",
    "        img_loc, lst_file_path = im2rec_lst(status, status_drop_train, dataset)\n",
    "        \n",
    "        print(\"Starting generating REC file: {} status - {} dataset\".format(status, dataset))\n",
    "        \n",
    "        im2rec_rec(img_loc, lst_file_path)\n",
    "        \n",
    "        print(\"REC File complete.\")\n",
    "        \n",
    "        write_hash(status, file_path, file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Flow Function\n",
    "The following function will verify the hash of the current files matches the older record. If they match, no action is taken. If they do not match, the input data is processed, and a new hash is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_hash(dataset, status):\n",
    "    \n",
    "    # Verify dataset variable is correct.\n",
    "    dataset_list = [\"sample\", \"complete\", \"train\", \"validation\", \"dummy\"]\n",
    "    \n",
    "    if dataset not in dataset_list:\n",
    "        print(\"Error. Correct Dataset Type Not Entered.\")\n",
    "        print(\"Dataset must be either type sample or complete.\")\n",
    "        return\n",
    "    \n",
    "    print()\n",
    "    print(\"Dataset: {}\".format(dataset))\n",
    "    print()\n",
    "            \n",
    "              \n",
    "    # Generate File Name, Path and Directory for Hash\n",
    "    status_drop_train = status.split(\"-\")[1]\n",
    "    file_name = \"hash_{}.csv\".format(dataset)\n",
    "    file_path_hash = os.path.join(\"hash\", status_drop_train, file_name)\n",
    "    file_dir = os.path.join('imgs', status, dataset)\n",
    "    \n",
    "    # Print for Sanity Check\n",
    "    print(\"Hash File Name: {}\".format(file_name))\n",
    "    print(\"Hash File Path: {}\".format(file_path_hash))\n",
    "    print(\"Image Set File Directory: {}\".format(file_dir))\n",
    "    print()\n",
    "    \n",
    "    # Generate Current Hash\n",
    "    print('New Hash - Generating')\n",
    "    hash_new = dirhash(file_dir, 'sha256')\n",
    "    print('New Hash - Generated')\n",
    "    \n",
    "    if not os.path.exists(file_path_hash):\n",
    "        print(\"Hash for {} images in {} do not exist.\".format(dataset, status))\n",
    "        print()\n",
    "        im2rec_and_write(status, status_drop_train, dataset, \n",
    "                         file_name, file_path_hash, file_dir)\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        # Read Existing Hash\n",
    "        df_hash_old = pd.read_csv(file_path_hash)\n",
    "        hash_old = df_hash_old.iloc[0]['hash']\n",
    "        \n",
    "        if hash_old == hash_new:\n",
    "            print(\"Hash for {} images are equal.\".format(status))\n",
    "            print(\"New Hash not generated. Processing not required.\")\n",
    "            print()\n",
    "            return\n",
    "        \n",
    "        else:\n",
    "            print(\"Hash for {} images are NOT equal.\".format(status))\n",
    "            print()\n",
    "            im2rec_and_write(status, status_drop_train, dataset, \n",
    "                             file_name, file_path_hash, file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Process\n",
    "Runnign the following code chuck will execute the process flow function develpoed above for both training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: train\n",
      "\n",
      "Hash File Name: hash_train.csv\n",
      "Hash File Path: hash\\split_driver\\hash_train.csv\n",
      "Image Set File Directory: imgs\\train-split_driver\\train\n",
      "\n",
      "New Hash - Generating\n",
      "New Hash - Generated\n",
      "Hash for train images in train-split_driver do not exist.\n",
      "\n",
      "Generating LST File: train-split_driver status - train dataset\n",
      "c0 0\n",
      "c1 1\n",
      "c2 2\n",
      "c3 3\n",
      "c4 4\n",
      "c5 5\n",
      "c6 6\n",
      "c7 7\n",
      "c8 8\n",
      "c9 9\n",
      "Starting generating REC file: train-split_driver status - train dataset\n",
      "Creating .rec file from D:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\model_input_files\\split_driver\\train.lst in D:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\model_input_files\\split_driver\n",
      "multiprocessing not available, fall back to single threaded encoding\n",
      "time: 0.005982398986816406  count: 0\n",
      "time: 6.443885564804077  count: 1000\n",
      "time: 8.041106939315796  count: 2000\n",
      "time: 6.454342603683472  count: 3000\n",
      "time: 6.0480637550354  count: 4000\n",
      "time: 7.386298656463623  count: 5000\n",
      "time: 6.475639343261719  count: 6000\n",
      "time: 6.188019752502441  count: 7000\n",
      "time: 6.808847188949585  count: 8000\n",
      "time: 7.586018085479736  count: 9000\n",
      "time: 6.502488374710083  count: 10000\n",
      "time: 6.099791526794434  count: 11000\n",
      "time: 8.227052927017212  count: 12000\n",
      "time: 8.187156438827515  count: 13000\n",
      "time: 6.7163262367248535  count: 14000\n",
      "time: 7.721201419830322  count: 15000\n",
      "time: 7.81109881401062  count: 16000\n",
      "time: 7.360410690307617  count: 17000\n",
      "Creating .rec file from D:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\model_input_files\\split_driver\\train_train.lst in D:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\model_input_files\\split_driver\n",
      "multiprocessing not available, fall back to single threaded encoding\n",
      "time: 0.007977962493896484  count: 0\n",
      "time: 7.349031925201416  count: 1000\n",
      "time: 8.316153764724731  count: 2000\n",
      "time: 7.137732267379761  count: 3000\n",
      "time: 6.982820510864258  count: 4000\n",
      "time: 8.810702323913574  count: 5000\n",
      "time: 7.125442981719971  count: 6000\n",
      "time: 7.244810342788696  count: 7000\n",
      "time: 8.216169357299805  count: 8000\n",
      "time: 7.874946117401123  count: 9000\n",
      "imread error trying to load file: D:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\imgs\\train-split_driver\\train\\c7\\img_74829.jpg \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\incubator-mxnet\\tools\\im2rec.py\", line 179, in image_encode\n",
      "    img = cv2.imread(fullpath, args.color)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mD:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\incubator-mxnet\\tools\\im2rec.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    384\u001b[0m                             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m                         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m                         \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                             \u001b[0mcur_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages\\mxnet\\recordio.py\u001b[0m in \u001b[0;36mwrite_idx\u001b[1;34m(self, idx, buf)\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfidx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s\\t%d\\n'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages\\mxnet\\recordio.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, buf)\u001b[0m\n\u001b[0;32m    133\u001b[0m         check_call(_LIB.MXRecordIOWriterWriteRecord(self.handle,\n\u001b[0;32m    134\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                                                     ctypes.c_size_t(len(buf))))\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REC File complete.\n",
      "Hash for train-split_driver images successfully written to CSV.\n",
      "\n",
      "\n",
      "Dataset: validation\n",
      "\n",
      "Hash File Name: hash_validation.csv\n",
      "Hash File Path: hash\\split_driver\\hash_validation.csv\n",
      "Image Set File Directory: imgs\\train-split_driver\\validation\n",
      "\n",
      "New Hash - Generating\n",
      "New Hash - Generated\n",
      "Hash for validation images in train-split_driver do not exist.\n",
      "\n",
      "Generating LST File: train-split_driver status - validation dataset\n",
      "c0 0\n",
      "c1 1\n",
      "c2 2\n",
      "c3 3\n",
      "c4 4\n",
      "c5 5\n",
      "c6 6\n",
      "c7 7\n",
      "c8 8\n",
      "c9 9\n",
      "Starting generating REC file: train-split_driver status - validation dataset\n",
      "Creating .rec file from D:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\model_input_files\\split_driver\\validation.lst in D:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\model_input_files\\split_driver\n",
      "multiprocessing not available, fall back to single threaded encoding\n",
      "time: 0.011969327926635742  count: 0\n",
      "time: 9.530853509902954  count: 1000\n",
      "time: 7.656746864318848  count: 2000\n",
      "time: 8.24235486984253  count: 3000\n",
      "time: 7.210391998291016  count: 4000\n",
      "imread error trying to load file: D:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\imgs\\train-split_driver\\validation\\c4\\img_21362.jpg \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\incubator-mxnet\\tools\\im2rec.py\", line 179, in image_encode\n",
      "    img = cv2.imread(fullpath, args.color)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mD:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\incubator-mxnet\\tools\\im2rec.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    384\u001b[0m                             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m                         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m                         \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                             \u001b[0mcur_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages\\mxnet\\recordio.py\u001b[0m in \u001b[0;36mwrite_idx\u001b[1;34m(self, idx, buf)\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfidx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s\\t%d\\n'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages\\mxnet\\recordio.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, buf)\u001b[0m\n\u001b[0;32m    133\u001b[0m         check_call(_LIB.MXRecordIOWriterWriteRecord(self.handle,\n\u001b[0;32m    134\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                                                     ctypes.c_size_t(len(buf))))\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REC File complete.\n",
      "Hash for train-split_driver images successfully written to CSV.\n",
      "\n",
      "\n",
      "Dataset: dummy\n",
      "\n",
      "Hash File Name: hash_dummy.csv\n",
      "Hash File Path: hash\\split_driver\\hash_dummy.csv\n",
      "Image Set File Directory: imgs\\train-split_driver\\dummy\n",
      "\n",
      "New Hash - Generating\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-a8c03479f70f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdriver_dataset_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mverify_hash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-111-a3053ba8ab9f>\u001b[0m in \u001b[0;36mverify_hash\u001b[1;34m(dataset, status)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Generate Current Hash\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'New Hash - Generating'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mhash_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirhash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sha256'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'New Hash - Generated'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages\\checksumdir\\__init__.py\u001b[0m in \u001b[0;36mdirhash\u001b[1;34m(dirname, hashfunc, excluded_files, ignore_hidden, followlinks, excluded_extensions)\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mhashvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_filehash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_reduce_hash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhashvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages\\checksumdir\\__init__.py\u001b[0m in \u001b[0;36m_filehash\u001b[1;34m(filepath, hashfunc)\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0mhasher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhasher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "driver_dataset_list = [\"train\", \"validation\", \"dummy\"]\n",
    "\n",
    "status = \"train-split_driver\"\n",
    "\n",
    "for dataset in driver_dataset_list:\n",
    "    verify_hash(dataset, status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish AWS Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = 'dsba_6190_proj_4'\n",
    "region = 'us-east-1'\n",
    "bucket = 'dsba-6190-final-team-project'\n",
    "prefix = \"channels_rec\"\n",
    "split_method = \"split_driver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session(profile_name = profile,\n",
    "                               region_name = region)\n",
    "\n",
    "s3_resource = session.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload\n",
    "We now upload the Recordio format files to S3. The Amazon Sagemaker Algorithm requires a specific folder format, with the training and validation data as seperate subfolders of the same directory, with the names train and validation, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(status, dataset):\n",
    "    # Establish Dynamic File and Path Names\n",
    "    folder = \"model_input_files\"\n",
    "    file_name_local = status + \"_split_driver_\"  + dataset + \".rec\"\n",
    "    file_name_s3 = status +  \".rec\"\n",
    "    \n",
    "    # Define Boto3 Resource Inputs\n",
    "    file_for_upload =  os.path.join(folder, split_method, file_name_local).replace('\\\\','/')\n",
    "    s3_key = os.path.join(prefix, split_method, dataset, status, file_name_s3).replace('\\\\','/')\n",
    "    \n",
    "    print(\"File for Upload:\\t{}\".format(file_for_upload))\n",
    "    print(\"s3 Key:\\t\\t\\t{}\".format(s3_key))\n",
    "    \n",
    "    # Upload\n",
    "    s3_resource.Bucket(bucket).upload_file(Filename = file_for_upload, Key = s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for Upload:\tmodel_input_files/split_driver/train_subset_complete.rec\n",
      "s3 Key:\t\t\tchannels/split_driver/complete/train/train.rec\n"
     ]
    }
   ],
   "source": [
    "status = status_list[0]\n",
    "dataset = dataset_list[0]\n",
    "\n",
    "#upload_to_s3(status, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for Upload:\tmodel_input_files/split_driver/validation_subset_complete.rec\n",
      "s3 Key:\t\t\tchannels/split_driver/complete/validation/validation.rec\n"
     ]
    }
   ],
   "source": [
    "status = status_list[1]\n",
    "dataset = dataset_list[0]\n",
    "\n",
    "#upload_to_s3(status, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for Upload:\tmodel_input_files/split_driver/train_subset_sample.rec\n",
      "s3 Key:\t\t\tchannels/split_driver/sample/train/train.rec\n"
     ]
    }
   ],
   "source": [
    "status = status_list[0]\n",
    "dataset = dataset_list[1]\n",
    "\n",
    "#upload_to_s3(status, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for Upload:\tmodel_input_files/split_driver/validation_subset_sample.rec\n",
      "s3 Key:\t\t\tchannels/split_driver/sample/validation/validation.rec\n"
     ]
    }
   ],
   "source": [
    "status = status_list[1]\n",
    "dataset = dataset_list[1]\n",
    "\n",
    "#upload_to_s3(status, dataset)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "State Farm Distracted Driver Process Flow.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
