{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "## Project Folder Structure\n",
    "The data for this file is too large to store with the github repo. Therefore, the files need to be added manually. The raw image files can be found here:\n",
    "\n",
    "[https://www.kaggle.com/c/state-farm-distracted-driver-detection/data](https://www.kaggle.com/c/state-farm-distracted-driver-detection/data) (Accessed on 3/21/2020)\n",
    "\n",
    "The file structre should be as follows:\n",
    "\n",
    "--Working\n",
    "\n",
    "----imgs\n",
    "\n",
    "------complete\n",
    "  \n",
    "--------train\n",
    "\n",
    "--------test\n",
    "\n",
    "The train and test folders come directly from the kaggle dataset. Subsequent folders will be generated by this notebook.\n",
    "\n",
    "## Scope\n",
    "This notebook is to prepare the images in the distracted driver dataset locally, for upload to S3.\n",
    "\n",
    "This notebook will create two versions of the data. The first will be a LST mapping file, with the image files in JPEG format. The second will be a RECORDIO format.\n",
    "\n",
    "A Complete dataset and Sample dataset will be created and uploaded.\n",
    "\n",
    "## Method\n",
    "\n",
    "This notebook splits the training and validation set randomly, using the **im2rec.py** train/validate function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone MXNET to get im2rec tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/apache/incubator-mxnet.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: checksumdir in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (1.1.7)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (4.2.0.32)\n",
      "Requirement already satisfied: mxnet in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from opencv-python) (1.15.0)\n",
      "Requirement already satisfied: requests<2.19.0,>=2.18.4 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from mxnet) (2.18.4)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2019.11.28)\n"
     ]
    }
   ],
   "source": [
    "!pip install checksumdir\n",
    "!pip install opencv-python mxnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1z9yTpGbACbX"
   },
   "source": [
    "## Library / Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhXJ8brV_4Qb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random \n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "\n",
    "import hashlib\n",
    "from checksumdir import dirhash\n",
    "\n",
    "from filecmp import dircmp\n",
    "\n",
    "#Settings\n",
    "seed = 5590"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "406FN849AG3C"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RpUao_lNAE88"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/DSBA-6190-Final-Project-Team/DSBA-6190_Final-Project/master/wine_predict/data/driver_imgs_list.csv\"\n",
    "path_file = 'data/driver_imgs_list.csv'\n",
    "\n",
    "df_driver_index = pd.read_csv(path_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xr04mwBdAwNY"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1W_1IwhLA7vT",
    "outputId": "6c263b25-d00d-46cc-c0cc-35a5f5e9f47c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22424, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_driver_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "n3glHqjBA3wI",
    "outputId": "d57b86d9-fe7b-4640-f241-3cba70bfab9e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_44733.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_72999.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_25094.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_69092.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_92629.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22419</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_56936.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22420</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_46218.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22421</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_25946.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22422</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_67850.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22423</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_9684.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22424 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject classname            img\n",
       "0        p002        c0  img_44733.jpg\n",
       "1        p002        c0  img_72999.jpg\n",
       "2        p002        c0  img_25094.jpg\n",
       "3        p002        c0  img_69092.jpg\n",
       "4        p002        c0  img_92629.jpg\n",
       "...       ...       ...            ...\n",
       "22419    p081        c9  img_56936.jpg\n",
       "22420    p081        c9  img_46218.jpg\n",
       "22421    p081        c9  img_25946.jpg\n",
       "22422    p081        c9  img_67850.jpg\n",
       "22423    p081        c9   img_9684.jpg\n",
       "\n",
       "[22424 rows x 3 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_driver_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R-sfjF4eAyMI",
    "outputId": "2a8fafe6-343c-4cc7-8b1c-5f64856d2518"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'classname', 'img'], dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_driver_index.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2w7VZnGBz8G"
   },
   "source": [
    "The following lists each unique driver along with the number of different classname and images are associated with each. Note the number of classnames is not unique, so the images an classnames have equal frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 886
    },
    "colab_type": "code",
    "id": "_vqngHEZBEVL",
    "outputId": "f391960d-5245-42b3-ba56-0cde324c5a5b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p002</th>\n",
       "      <td>725</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p012</th>\n",
       "      <td>823</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p014</th>\n",
       "      <td>876</td>\n",
       "      <td>876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p015</th>\n",
       "      <td>875</td>\n",
       "      <td>875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p016</th>\n",
       "      <td>1078</td>\n",
       "      <td>1078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p021</th>\n",
       "      <td>1237</td>\n",
       "      <td>1237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p022</th>\n",
       "      <td>1233</td>\n",
       "      <td>1233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p024</th>\n",
       "      <td>1226</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p026</th>\n",
       "      <td>1196</td>\n",
       "      <td>1196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p035</th>\n",
       "      <td>848</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p039</th>\n",
       "      <td>651</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p041</th>\n",
       "      <td>605</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p042</th>\n",
       "      <td>591</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p045</th>\n",
       "      <td>724</td>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p047</th>\n",
       "      <td>835</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p049</th>\n",
       "      <td>1011</td>\n",
       "      <td>1011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p050</th>\n",
       "      <td>790</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p051</th>\n",
       "      <td>920</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p052</th>\n",
       "      <td>740</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p056</th>\n",
       "      <td>794</td>\n",
       "      <td>794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p061</th>\n",
       "      <td>809</td>\n",
       "      <td>809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p064</th>\n",
       "      <td>820</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p066</th>\n",
       "      <td>1034</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p072</th>\n",
       "      <td>346</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p075</th>\n",
       "      <td>814</td>\n",
       "      <td>814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p081</th>\n",
       "      <td>823</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         classname   img\n",
       "subject                 \n",
       "p002           725   725\n",
       "p012           823   823\n",
       "p014           876   876\n",
       "p015           875   875\n",
       "p016          1078  1078\n",
       "p021          1237  1237\n",
       "p022          1233  1233\n",
       "p024          1226  1226\n",
       "p026          1196  1196\n",
       "p035           848   848\n",
       "p039           651   651\n",
       "p041           605   605\n",
       "p042           591   591\n",
       "p045           724   724\n",
       "p047           835   835\n",
       "p049          1011  1011\n",
       "p050           790   790\n",
       "p051           920   920\n",
       "p052           740   740\n",
       "p056           794   794\n",
       "p061           809   809\n",
       "p064           820   820\n",
       "p066          1034  1034\n",
       "p072           346   346\n",
       "p075           814   814\n",
       "p081           823   823"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drivers_gb = df_driver_index.groupby(['subject'])\n",
    "drivers_gb.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Sampled Subset of Images\n",
    "\n",
    "When using the **im2rec.py** tool to create RECORDIO files, to create the 10% sample, we need to create a copy of the raw images. \n",
    "\n",
    "## Create Sampled Dataframe\n",
    "To do this, we can make a dataframe listing every image with the images relative location. Then take a sample of the dataframe. With the sub-sampled dataframe we can copy the sampled set. We start with the master list of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_44733.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_72999.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_25094.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_69092.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_92629.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject classname            img\n",
       "0    p002        c0  img_44733.jpg\n",
       "1    p002        c0  img_72999.jpg\n",
       "2    p002        c0  img_25094.jpg\n",
       "3    p002        c0  img_69092.jpg\n",
       "4    p002        c0  img_92629.jpg"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_driver_index.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to sample from each class. To do so we'll group by class, then apply sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classname</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">c0</th>\n",
       "      <th>8160</th>\n",
       "      <td>p026</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_87669.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>p016</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_67547.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20854</th>\n",
       "      <td>p075</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_43775.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15331</th>\n",
       "      <td>p051</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_17811.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15477</th>\n",
       "      <td>p051</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_99652.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">c9</th>\n",
       "      <th>12635</th>\n",
       "      <td>p045</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_3155.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7987</th>\n",
       "      <td>p024</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_59938.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>p024</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_80769.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6728</th>\n",
       "      <td>p022</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_50864.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21540</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_84168.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2243 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                subject classname            img\n",
       "classname                                       \n",
       "c0        8160     p026        c0  img_87669.jpg\n",
       "          3328     p016        c0  img_67547.jpg\n",
       "          20854    p075        c0  img_43775.jpg\n",
       "          15331    p051        c0  img_17811.jpg\n",
       "          15477    p051        c0  img_99652.jpg\n",
       "...                 ...       ...            ...\n",
       "c9        12635    p045        c9   img_3155.jpg\n",
       "          7987     p024        c9  img_59938.jpg\n",
       "          7999     p024        c9  img_80769.jpg\n",
       "          6728     p022        c9  img_50864.jpg\n",
       "          21540    p075        c9  img_84168.jpg\n",
       "\n",
       "[2243 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_driver_index_sample = df_driver_index.groupby('classname').apply(pd.DataFrame.sample, \n",
    "                                                                    random_state = seed,  \n",
    "                                                                    frac=0.1)\n",
    "\n",
    "df_driver_index_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy\n",
    "With the sampled dataframe, we can iterate through the list, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_imgs_im2rec(status, df):\n",
    "    prefix_subfolder = \"imgs\"\n",
    "    prefix_src = \"raw/train\"\n",
    "    prefix_dst = \"train-split_im2rec\"\n",
    "    #prefix_status = os.path.join(\"imgs\",folder_name)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        path_src = os.path.join(prefix_subfolder,\n",
    "                                prefix_src,\n",
    "                                row['classname'], \n",
    "                                row['img']).replace('\\\\','/')\n",
    "             \n",
    "       \n",
    "        path_dst = os.path.join(prefix_subfolder,\n",
    "                                prefix_dst,\n",
    "                                status,\n",
    "                                row['classname'],\n",
    "                                row['img']).replace('\\\\','/')\n",
    "        \n",
    "        #print(\"Source File:\\t\\t{}\".format(path_src))\n",
    "        #print(\"Destination File:\\t{}\".format(path_dst))\n",
    "    \n",
    "        \n",
    "        if not os.path.exists(path_dst):\n",
    "            # Verify Directory Exists. Create if Not \n",
    "            os.makedirs(os.path.dirname(path_dst), exist_ok=True)\n",
    "            \n",
    "            shutil.copy(path_src, path_dst)\n",
    "            #print(\"Loop Start\")\n",
    "            #print(\"Source File:\\t\\t{}\".format(path_src))\n",
    "            #print(\"Destination File:\\t{}\".format(path_dst))\n",
    "            #print()\n",
    "        #else:\n",
    "            #print(\"File Exists. No Copy Made.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we execute the function for both the complete data and the sample data.\n",
    "\n",
    "**Note:** I have been having issues when generating RECORDIO files in a loop. The last files generated in the loop appear to remain open by python until a python session is restarted. I'm worried they are being corrupted in some way. So, to try and avoid this, I am creating a third dummy dataset, to run at the end of the loop. This data will not be used in the analysis. It is only there to be the last action in the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_input = {\n",
    "    \"complete\": df_driver_index,\n",
    "    \"sample\" : df_driver_index_sample,\n",
    "    \"dummy\" : df_driver_index_sample\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_input:\n",
    "    copy_imgs_im2rec(key, dict_input[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Folder Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now verify the size of the resulting folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calculates the number of files in a directory, recursively\n",
    "def num_files(dir_scan):\n",
    "    cpt = sum([len(files) for r, d, files in os.walk(dir_scan)])\n",
    "    print(\"Directory Analyzed:\\t{}\".format(dir_scan))\n",
    "    print(\"\\t\\t\\tThere are {} image files in the directory.\".format(cpt))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_current = \"D:\\\\Notebooks\\\\dsba_6190\\\\team_project\\\\image_classification_preprocessing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Analyzed:\tD:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\imgs\\train-split_im2rec\\complete\n",
      "\t\t\tThere are 22424 image files in the directory.\n",
      "\n",
      "Directory Analyzed:\tD:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\imgs\\train-split_im2rec\\sample\n",
      "\t\t\tThere are 2243 image files in the directory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prefix_im2rec = \"imgs\\\\train-split_im2rec\"\n",
    "\n",
    "# Complete\n",
    "dir_complete = os.path.join(path_current, prefix_im2rec, \"complete\")\n",
    "num_files(dir_complete)\n",
    "\n",
    "# Sample\n",
    "dir_sample = os.path.join(path_current, prefix_im2rec, \"sample\")\n",
    "num_files(dir_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Flow\n",
    "\n",
    "Some of the downstream processing steps are computationally heavy. Therefore, in order to avoid running these code chunks, without manually commenting them out once run. \n",
    "\n",
    "To check if any of the input files have changed, which would then require reprocessing the images, we will use Hash checksum values to document the state of the inputs. We will check against this state to determine if we need to reprocess the data. \n",
    "\n",
    "To do this, we will have to convert the processing steps into callable functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## im2rec Function Calls\n",
    "The following functions use the im2rec.py tool to process the image inputs.\n",
    "\n",
    "[https://mxnet.apache.org/api/faq/recordio](https://mxnet.apache.org/api/faq/recordio) (Accessed on 3/20/2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LST File Generation\n",
    "This function performs the first step, creating LST mapping files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2rec_lst(status, dataset):\n",
    "    lst_file_name = dataset\n",
    "    lst_file_path = os.path.join(\"model_input_files\", \"split_im2rec\", dataset)\n",
    "    img_loc =  os.path.join(\"imgs\", status, dataset)\n",
    "    \n",
    "    %run incubator-mxnet/tools/im2rec.py --list --train-ratio 0.8  --recursive {lst_file_path} {img_loc} \n",
    "    \n",
    "    return img_loc, lst_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recordio Conversion\n",
    "This function converts the images into a RECORDIO binary file format. In addition to converting images from raw image files to RECORDIO, we need to resize the images. In the Amazon Sagemaker Image Classification Hyperparameters documentation ([here](https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html)) there is a parameter called **image_shape**. This parameter requires a string with three numbers, comma seperated (i.e. \"1,2,3\").\n",
    "\n",
    "The first value is **num_channels**. This is the number of channels our input images have. They are color images, so they have three channels (Red, Green, and Blue aka RGB). The second and third values are the heigth and width of the images, in pixels. Technically, the algorithm can accomadate images of any size, but if the images are too large, there may be memory constraints. It indicates typical dimensions are 244 x 244. Our images come are 640 x 480. By area this is 5x bigger. So, we need to resize the images.\n",
    "\n",
    "The tool **im2rec.py** is capable of resizing images during the RECORDIO conversion. All we need to to is add the argument \"**--resize #**\" to the command line entry. The number in the command line argument is what the tool will resize the ***shortest edge*** of the input image. The larger edge will be resized to maintain the same relative dimensions. \n",
    "\n",
    "Therefore, we are going to resize the input images so that the resulting area is approximately equal to the area of a 244 x 244 image. The dimensions we are going to use is 210 x 280. This maintains the relative size of the input images, while have an area 98.7% that of the default image size. So, in the command line argument, the call will be **--resize 210**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2rec_rec(img_loc, lst_file_path):\n",
    "    \n",
    "    %run incubator-mxnet/tools/im2rec.py  --resize 210 {lst_file_path} {img_loc} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Hash Checksum\n",
    "The following function generates the Hash of a directory (training or validation images) and writes the Hash in a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_hash(status, file_name, file_path, file_dir):\n",
    "\n",
    "    # Generate Hash\n",
    "    dir_hash = dirhash(file_dir, 'sha256')\n",
    "    \n",
    "    # Write to CSV\n",
    "    dict_dir_hash = {'hash': [dir_hash]}\n",
    "    df_dir_hash = pd.DataFrame(dict_dir_hash)\n",
    "    df_dir_hash.to_csv(file_path, index=False)\n",
    "    print(\"Hash for {} images successfully written to CSV.\".format(status))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Processing and Writing Hash\n",
    "Whenever we process the input data we need to write a new hash, and vice versa. Therefore, for simplicity in the final code, we will lump these actions together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2rec_and_write(status, dataset, file_name, file_path, file_dir):\n",
    "        print(\"Generating LST File: {} status - {} dataset\".format(status, dataset))\n",
    "        \n",
    "        img_loc, lst_file_path = im2rec_lst(status, dataset)\n",
    "        \n",
    "        print(\"Starting generating REC file: {} status - {} dataset\".format(status, dataset))\n",
    "        \n",
    "        im2rec_rec(img_loc, lst_file_path)\n",
    "        \n",
    "        print(\"REC File complete.\")\n",
    "        \n",
    "        write_hash(status, file_name, file_path, file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Flow Function\n",
    "The following function will verify the hash of the current files matches the older record. If they match, no action is taken. If they do not match, the input data is processed, and a new hash is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_hash_im2rec(dataset):\n",
    "    status = \"train-split_im2rec\"\n",
    "    \n",
    "    # Verify dataset variable is correct.\n",
    "    dataset_list = [\"sample\", \"complete\", \"dummy\"]\n",
    "    \n",
    "    if dataset not in dataset_list:\n",
    "        print(\"Error. Correct Dataset Type Not Entered.\")\n",
    "        print(\"Dataset must be either type sample or complete.\")\n",
    "        return\n",
    "    \n",
    "    print()\n",
    "    print(\"Dataset: {}\".format(dataset))\n",
    "    print()\n",
    "            \n",
    "              \n",
    "    # Generate File Name, Path and Directory for Hash \n",
    "    file_name = \"hash_{}.csv\".format(dataset)\n",
    "    file_path_hash = os.path.join(\"hash\", \"split_im2rec\", file_name)\n",
    "    file_dir = os.path.join('imgs', status, dataset)\n",
    "    \n",
    "    # Print for Sanity Check\n",
    "    print(\"Hash File Name: {}\".format(file_name))\n",
    "    print(\"Hash File Path: {}\".format(file_path_hash))\n",
    "    print(\"Image Set File Directory: {}\".format(file_dir))\n",
    "    print()\n",
    "    \n",
    "    # Generate Current Hash\n",
    "    print('New Hash - Generating')\n",
    "    hash_new = dirhash(file_dir, 'sha256')\n",
    "    print('New Hash - Generated')\n",
    "    \n",
    "    if not os.path.exists(file_path_hash):\n",
    "        print(\"Hash for {} images in {} do not exist.\".format(dataset, status))\n",
    "        print()\n",
    "        im2rec_and_write(status, dataset, file_name, file_path_hash, file_dir)\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        # Read Existing Hash\n",
    "        df_hash_old = pd.read_csv(file_path_hash)\n",
    "        hash_old = df_hash_old.iloc[0]['hash']\n",
    "        \n",
    "        if hash_old == hash_new:\n",
    "            print(\"Hash for {} images are equal.\".format(status))\n",
    "            print(\"New Hash not generated. Processing not required.\")\n",
    "            print()\n",
    "            return\n",
    "        \n",
    "        else:\n",
    "            print(\"Hash for {} images are NOT equal.\".format(status))\n",
    "            print()\n",
    "            im2rec_and_write_complete(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Process\n",
    "Runnign the following code chuck will execute the process flow function develpoed above for both training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: complete\n",
      "\n",
      "Hash File Name: hash_complete.csv\n",
      "Hash File Path: hash\\split_im2rec\\hash_complete.csv\n",
      "Image Set File Directory: imgs\\train-split_im2rec\\complete\n",
      "\n",
      "New Hash - Generating\n",
      "New Hash - Generated\n",
      "Hash for train-split_im2rec images are equal.\n",
      "New Hash not generated. Processing not required.\n",
      "\n",
      "\n",
      "Dataset: sample\n",
      "\n",
      "Hash File Name: hash_sample.csv\n",
      "Hash File Path: hash\\split_im2rec\\hash_sample.csv\n",
      "Image Set File Directory: imgs\\train-split_im2rec\\sample\n",
      "\n",
      "New Hash - Generating\n",
      "New Hash - Generated\n",
      "Hash for train-split_im2rec images are equal.\n",
      "New Hash not generated. Processing not required.\n",
      "\n",
      "\n",
      "Dataset: dummy\n",
      "\n",
      "Hash File Name: hash_dummy.csv\n",
      "Hash File Path: hash\\split_im2rec\\hash_dummy.csv\n",
      "Image Set File Directory: imgs\\train-split_im2rec\\dummy\n",
      "\n",
      "New Hash - Generating\n",
      "New Hash - Generated\n",
      "Hash for train-split_im2rec images are equal.\n",
      "New Hash not generated. Processing not required.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_list = [\"complete\", \"sample\", \"dummy\"]\n",
    "\n",
    "for dataset in dataset_list:\n",
    "    verify_hash_im2rec(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish AWS Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = 'dsba_6190_proj_4'\n",
    "region = 'us-east-1'\n",
    "bucket = 'dsba-6190-final-team-project'\n",
    "prefix_1 = \"channels\"\n",
    "prefix_2 = \"rec\"\n",
    "split_method = \"split_im2rec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session(profile_name = profile,\n",
    "                               region_name = region)\n",
    "\n",
    "s3_resource = session.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload\n",
    "We now upload the Recordio format files to S3. The Amazon Sagemaker Algorithm requires a specific folder format, with the training and validation data as seperate subfolders of the same directory, with the names train and validation, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(status, status_tag, dataset):\n",
    "    # Establish Dynamic File and Path Names\n",
    "    folder = \"model_input_files\"\n",
    "    file_name_local = dataset + \"_\"  + status_tag + \".rec\"\n",
    "    file_name_s3 = status +  \".rec\"\n",
    "    \n",
    "    # Define Boto3 Resource Inputs\n",
    "    file_for_upload =  os.path.join(folder, split_method, file_name_local).replace('\\\\','/')\n",
    "    s3_key = os.path.join(prefix_1, prefix_2, split_method, dataset, status, file_name_s3).replace('\\\\','/')\n",
    "    \n",
    "    print(\"File for Upload:\\t{}\".format(file_for_upload))\n",
    "    print(\"s3 Key:\\t\\t\\t{}\".format(s3_key))\n",
    "    \n",
    "    # Upload\n",
    "    s3_resource.Bucket(bucket).upload_file(Filename = file_for_upload, Key = s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for Upload:\tmodel_input_files/split_im2rec/complete_train.rec\n",
      "s3 Key:\t\t\tchannels/rec/split_im2rec/complete/train/train.rec\n"
     ]
    }
   ],
   "source": [
    "status = \"train\"\n",
    "statust_tag = status\n",
    "dataset = dataset_list[0]\n",
    "\n",
    "#upload_to_s3(status, statust_tag, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for Upload:\tmodel_input_files/split_im2rec/complete_val.rec\n",
      "s3 Key:\t\t\tchannels/rec/split_im2rec/complete/validation/validation.rec\n"
     ]
    }
   ],
   "source": [
    "status = \"validation\"\n",
    "statust_tag = \"val\"\n",
    "dataset = dataset_list[0]\n",
    "\n",
    "#upload_to_s3(status, statust_tag, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for Upload:\tmodel_input_files/split_im2rec/sample_train.rec\n",
      "s3 Key:\t\t\tchannels/rec/split_im2rec/sample/train/train.rec\n"
     ]
    }
   ],
   "source": [
    "status = \"train\"\n",
    "statust_tag = status\n",
    "dataset = dataset_list[1]\n",
    "\n",
    "#upload_to_s3(status, statust_tag, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for Upload:\tmodel_input_files/split_im2rec/sample_val.rec\n",
      "s3 Key:\t\t\tchannels/rec/split_im2rec/sample/validation/validation.rec\n"
     ]
    }
   ],
   "source": [
    "status = \"validation\"\n",
    "statust_tag = \"val\"\n",
    "dataset = dataset_list[1]\n",
    "\n",
    "#upload_to_s3(status, statust_tag, dataset)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "State Farm Distracted Driver Process Flow.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
