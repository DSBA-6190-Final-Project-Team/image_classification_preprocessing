{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "## Project Folder Structure\n",
    "The input data for this project is too large to store with the github repo. Therefore, the files need to be added manually. The raw image files can be found here:\n",
    "\n",
    "[https://www.kaggle.com/c/state-farm-distracted-driver-detection/data](https://www.kaggle.com/c/state-farm-distracted-driver-detection/data) (Accessed on 3/21/2020)\n",
    "\n",
    "You can download the raw input files as a zip file. It will contain the following:\n",
    "\n",
    "\n",
    "* driver_imgs_list.csv (This file in maintained in the repo)\n",
    "* sample_submission.csv (This file is maintained in the repo)\n",
    "* imgs folder, with test and train subfolders\n",
    "\n",
    "This **imgs** folder is the part of the input which is not maintained in the repo. Therefore, for this notebook to function the data in the downloaded **imgs** must be copied to the correct location. Copy the **test** and **train** subfolders to the location *Working Directory > imgs > kaggle*. The resulting folders should have the following structure:\n",
    "\n",
    "+--Working\n",
    "\n",
    "|_ +--imgs\n",
    "\n",
    "|_ _ +--kaggle\n",
    "  \n",
    "|_ _ _ +--train\n",
    "\n",
    "|_ _ _ +--test\n",
    "\n",
    "\n",
    "## Note on Dummy Data\n",
    "There are several locations in this notebook where dummy data is created. This is due to an issue I have not been able to figure out regarding creating files and folders in python over a loop. When I create new files using a loop, the files create during the last iteration of the loop are corrupted. Therefore, I have created a dummy placeholder, which will always go last in the loop. The loop will still create a corrupted file, but this file will be our dummy file. The files we need for our analysis will now not be corrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a python script called **im2rec.py** as a tool to process our input data. This python script is hosted on the Apache GitHub page. To ensure we have the most current version of this tool, we will clone the required GitHub repo into this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/apache/incubator-mxnet.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n",
    "The following packages were not available and needed to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: checksumdir in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (1.1.7)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (4.2.0.32)\n",
      "Requirement already satisfied: mxnet in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from opencv-python) (1.15.0)\n",
      "Requirement already satisfied: requests<2.19.0,>=2.18.4 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from mxnet) (2.18.4)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2019.11.28)\n"
     ]
    }
   ],
   "source": [
    "!pip install checksumdir\n",
    "!pip install opencv-python mxnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1z9yTpGbACbX"
   },
   "source": [
    "## Library / Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhXJ8brV_4Qb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "\n",
    "import hashlib\n",
    "from checksumdir import dirhash\n",
    "\n",
    "from filecmp import dircmp\n",
    "\n",
    "#Settings\n",
    "seed = 5590"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "406FN849AG3C"
   },
   "source": [
    "## Data\n",
    "See the Note at the top tof this notebook for how to use the raw input image files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RpUao_lNAE88"
   },
   "outputs": [],
   "source": [
    "df_driver_index = pd.read_csv(\"data/driver_imgs_list.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xr04mwBdAwNY"
   },
   "source": [
    "# EDA\n",
    "## Overall Data Shape\n",
    "The following section is some general Exploratory Data Analysis, focused on the image list.\n",
    "\n",
    "The following are the columns in the image list CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'classname', 'img'], dtype='object')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_driver_index.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1W_1IwhLA7vT",
    "outputId": "6c263b25-d00d-46cc-c0cc-35a5f5e9f47c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Images: (22424, 3)\n"
     ]
    }
   ],
   "source": [
    "num_images = df_driver_index.shape\n",
    "print(\"Number of Images: {}\".format(num_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes: 10\n"
     ]
    }
   ],
   "source": [
    "num_classes = df_driver_index['classname'].nunique()\n",
    "print(\"Number of Classes: {}\".format(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2w7VZnGBz8G"
   },
   "source": [
    "The following lists each unique driver along with the number of different classname and images are associated with each. Note the number of classnames is not unique, so the images an classnames have equal frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 886
    },
    "colab_type": "code",
    "id": "_vqngHEZBEVL",
    "outputId": "f391960d-5245-42b3-ba56-0cde324c5a5b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p002</th>\n",
       "      <td>725</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p012</th>\n",
       "      <td>823</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p014</th>\n",
       "      <td>876</td>\n",
       "      <td>876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p015</th>\n",
       "      <td>875</td>\n",
       "      <td>875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p016</th>\n",
       "      <td>1078</td>\n",
       "      <td>1078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p021</th>\n",
       "      <td>1237</td>\n",
       "      <td>1237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p022</th>\n",
       "      <td>1233</td>\n",
       "      <td>1233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p024</th>\n",
       "      <td>1226</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p026</th>\n",
       "      <td>1196</td>\n",
       "      <td>1196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p035</th>\n",
       "      <td>848</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p039</th>\n",
       "      <td>651</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p041</th>\n",
       "      <td>605</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p042</th>\n",
       "      <td>591</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p045</th>\n",
       "      <td>724</td>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p047</th>\n",
       "      <td>835</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p049</th>\n",
       "      <td>1011</td>\n",
       "      <td>1011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p050</th>\n",
       "      <td>790</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p051</th>\n",
       "      <td>920</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p052</th>\n",
       "      <td>740</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p056</th>\n",
       "      <td>794</td>\n",
       "      <td>794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p061</th>\n",
       "      <td>809</td>\n",
       "      <td>809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p064</th>\n",
       "      <td>820</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p066</th>\n",
       "      <td>1034</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p072</th>\n",
       "      <td>346</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p075</th>\n",
       "      <td>814</td>\n",
       "      <td>814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p081</th>\n",
       "      <td>823</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         classname   img\n",
       "subject                 \n",
       "p002           725   725\n",
       "p012           823   823\n",
       "p014           876   876\n",
       "p015           875   875\n",
       "p016          1078  1078\n",
       "p021          1237  1237\n",
       "p022          1233  1233\n",
       "p024          1226  1226\n",
       "p026          1196  1196\n",
       "p035           848   848\n",
       "p039           651   651\n",
       "p041           605   605\n",
       "p042           591   591\n",
       "p045           724   724\n",
       "p047           835   835\n",
       "p049          1011  1011\n",
       "p050           790   790\n",
       "p051           920   920\n",
       "p052           740   740\n",
       "p056           794   794\n",
       "p061           809   809\n",
       "p064           820   820\n",
       "p066          1034  1034\n",
       "p072           346   346\n",
       "p075           814   814\n",
       "p081           823   823"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drivers_gb = df_driver_index.groupby(['subject'])\n",
    "drivers_gb.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Validation Data Split\n",
    "While the imported data from **Kaggle** contains a **train** and **test** set of images, the **test** set in unable to funciton as our test or validation set. The images in the **test** folder are unlabeled, so we do not know what class they are in. These images function more as a **Kaggle** scoring test set.\n",
    "\n",
    "In order to correctly train our image classification model, we will need a proper training and validation test set. We will pursue two different train/validation split methodologies. \n",
    "\n",
    "1. **Random Split**: The first method will be a general random split of the images. We will use the **im2rec.py** tool provided by Apache in the MXNET project folder. With this tool all we need to do is supply the image folder location and the train/test split, and the tool will create the two sets of images automatically.\n",
    "2. **Driver Split**: This method is based on a blog post on this data that I can no longer find, unfortunatley. Instead of splitting randomly on the images, we will split the drivers randomly. Then, all of the images associated with a driver will either be in the train set, or validation set. No driver will appear in both sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 - Random Split\n",
    "As stated before, the **im2rec.py** tool will handle the training/validation split for us. But we sill need to create new sets of images. We will create three image sets:\n",
    "\n",
    "1. Full copy of complete data\n",
    "2. Sampling of comlpete data (10% by class)\n",
    "3. Dummy data, equal to the 10% sample (See top of notebook for a note on dummy data)\n",
    "\n",
    "To create a 10% sample, we need to create a sample list of images. We do that by using the overall image list, grouping by the class, and then applying a random sample function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classname</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">c0</th>\n",
       "      <th>8160</th>\n",
       "      <td>p026</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_87669.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>p016</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_67547.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20854</th>\n",
       "      <td>p075</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_43775.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15331</th>\n",
       "      <td>p051</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_17811.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15477</th>\n",
       "      <td>p051</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_99652.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">c9</th>\n",
       "      <th>12635</th>\n",
       "      <td>p045</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_3155.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7987</th>\n",
       "      <td>p024</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_59938.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>p024</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_80769.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6728</th>\n",
       "      <td>p022</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_50864.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21540</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_84168.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2243 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                subject classname            img\n",
       "classname                                       \n",
       "c0        8160     p026        c0  img_87669.jpg\n",
       "          3328     p016        c0  img_67547.jpg\n",
       "          20854    p075        c0  img_43775.jpg\n",
       "          15331    p051        c0  img_17811.jpg\n",
       "          15477    p051        c0  img_99652.jpg\n",
       "...                 ...       ...            ...\n",
       "c9        12635    p045        c9   img_3155.jpg\n",
       "          7987     p024        c9  img_59938.jpg\n",
       "          7999     p024        c9  img_80769.jpg\n",
       "          6728     p022        c9  img_50864.jpg\n",
       "          21540    p075        c9  img_84168.jpg\n",
       "\n",
       "[2243 rows x 3 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_driver_index_sample = df_driver_index.groupby('classname').apply(pd.DataFrame.sample, \n",
    "                                                                    random_state = seed,  \n",
    "                                                                    frac=0.1)\n",
    "\n",
    "df_driver_index_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 - Driver Split\n",
    "The following section splits the drivers into training and validation sets.\n",
    "\n",
    "First, we will set the training/validation split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SNDV8uWICp9y"
   },
   "outputs": [],
   "source": [
    "train_val_split = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6gBnk_kDXtt"
   },
   "source": [
    "To split the data into training and validation sets, we'll need a unique index of drivers. We don't need the frequency counts.\n",
    "\n",
    "We will create a shuffled list of unique drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "5Lps82L_Dn9-",
    "outputId": "f0aff480-dc8e-471a-d45f-ee9fd2f8d09e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p049', 'p022', 'p012', 'p075', 'p045', 'p072', 'p026', 'p042', 'p015', 'p039', 'p047', 'p081', 'p052', 'p014', 'p021', 'p035', 'p061', 'p064', 'p066', 'p056', 'p002', 'p041', 'p024', 'p016', 'p051', 'p050']\n"
     ]
    }
   ],
   "source": [
    "drivers_unique = drivers_gb.groups.keys()\n",
    "drivers_unique = list(drivers_unique)\n",
    "\n",
    "random.Random(seed).shuffle(drivers_unique)\n",
    "print(drivers_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YOClOK2lEZsD"
   },
   "source": [
    "We'll set the list of drivers in the train set and the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rzmgBKnHEfmx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Drivers - Training Set:\t21\n",
      "Number of Drivers - Validation Set:\t5\n"
     ]
    }
   ],
   "source": [
    "num_drivers_val = round(len(drivers_unique)*train_val_split)\n",
    "num_drivers_train = len(drivers_unique) - num_drivers_val\n",
    "\n",
    "#Print\n",
    "print(\"Number of Drivers - Training Set:\\t{}\".format(num_drivers_train))\n",
    "print(\"Number of Drivers - Validation Set:\\t{}\".format(num_drivers_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aA1httPIFte_",
    "outputId": "e23ed327-f12f-408b-bf00-8cfeb1d49f4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p072', 'p026', 'p042', 'p015', 'p039', 'p047', 'p081', 'p052', 'p014', 'p021', 'p035', 'p061', 'p064', 'p066', 'p056', 'p002', 'p041', 'p024', 'p016', 'p051', 'p050']\n"
     ]
    }
   ],
   "source": [
    "drivers_train = drivers_unique[-num_drivers_train:]\n",
    "print(drivers_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p049', 'p022', 'p012', 'p075', 'p045']\n"
     ]
    }
   ],
   "source": [
    "drivers_val = drivers_unique[:num_drivers_val]\n",
    "print(drivers_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hNA4cd_cPquX"
   },
   "source": [
    "### Training and Validation Image Lists\n",
    "\n",
    "With the training and validation sets of drivers established, we will now create two lists of images. One list will be of every image file name associated with the trainging set, the other list associated with the validation set. \n",
    "\n",
    "These lists will be used to copy from the **Kaggle** dataset to create unique datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f8a4uhBKP-uQ"
   },
   "outputs": [],
   "source": [
    "df_images_val = df_driver_index[df_driver_index['subject'].isin(drivers_val)]\n",
    "df_images_train = df_driver_index[~df_driver_index['subject'].isin(drivers_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "vHXC2dsXQ2Po",
    "outputId": "d2f863d9-7bdb-493f-d31e-148314e19378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17819, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_44733.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_72999.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_25094.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_69092.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_92629.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject classname            img\n",
       "0    p002        c0  img_44733.jpg\n",
       "1    p002        c0  img_72999.jpg\n",
       "2    p002        c0  img_25094.jpg\n",
       "3    p002        c0  img_69092.jpg\n",
       "4    p002        c0  img_92629.jpg"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_images_train.shape)\n",
    "df_images_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "ex0G4NQCRn0j",
    "outputId": "cc7473cf-2acc-41b2-aa75-507a59349d7c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4605, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_10206.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_27079.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_50749.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_97089.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_37741.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject classname            img\n",
       "725    p012        c0  img_10206.jpg\n",
       "726    p012        c0  img_27079.jpg\n",
       "727    p012        c0  img_50749.jpg\n",
       "728    p012        c0  img_97089.jpg\n",
       "729    p012        c0  img_37741.jpg"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_images_val.shape)\n",
    "df_images_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Images\n",
    "We need to create two folders images, one for training, and one for validation. We will leave the **Kaggle** folder of images alone, so we always have the raw data if we need it. \n",
    "\n",
    "We will create these new folders by copying from the **Kaggle** folder, using the lists of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Copy\n",
    "The following function takes the training or validaiton dataframe and iterates through each row. It then copies each file from the overall data to the respective folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_imgs(nested_dict, folder = \"default\"):\n",
    "   \n",
    "    for key, value in nested_dict.items():\n",
    "        print(\"Entering Initial For Loop\")\n",
    "        print(\"\")\n",
    "        if type(value) is dict:\n",
    "            print(\"Entering If Statment - Recursive\")\n",
    "            print(\"\")\n",
    "            # Define Local Values\n",
    "            folder = key\n",
    "            copy_imgs(value, folder = folder)\n",
    "           \n",
    "        else:\n",
    "            print(\"Entering Else Statment\")\n",
    "            print(\"Folder: {}\".format(folder))\n",
    "            print(\"Key: {}\".format(key))\n",
    "            print(\"\")\n",
    "            prefix_subfolder = \"imgs\"\n",
    "            prefix_src = \"kaggle/train\"\n",
    "            prefix_dst = \"train-\" + folder\n",
    "    \n",
    "            for index, row in value.iterrows():\n",
    "                path_src = os.path.join(prefix_subfolder,\n",
    "                                        prefix_src,\n",
    "                                        row['classname'], \n",
    "                                        row['img']).replace('\\\\','/')\n",
    "\n",
    "                path_dst = os.path.join(prefix_subfolder,\n",
    "                            prefix_dst,\n",
    "                            key,\n",
    "                            row['classname'],\n",
    "                            row['img']).replace('\\\\','/')\n",
    "                \n",
    "                #print(\"Source File:\\t\\t{}\".format(path_src))\n",
    "                #print(\"Destination File:\\t{}\".format(path_dst))\n",
    "    \n",
    "        \n",
    "                if not os.path.exists(path_dst):\n",
    "                    # Verify Directory Exists. Create if Not \n",
    "                    os.makedirs(os.path.dirname(path_dst), exist_ok=True)\n",
    "\n",
    "                    shutil.copy(path_src, path_dst)\n",
    "                    #print(\"Loop Start\")\n",
    "                    #print(\"Source File:\\t\\t{}\".format(path_src))\n",
    "                    #print(\"Destination File:\\t{}\".format(path_dst))\n",
    "                    #print()\n",
    "                #else:\n",
    "                    #print(\"File Exists. No Copy Made.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a nested list of dictionaries to loop over to execute the required copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Split Dictionary\n",
    "dict_random_split = {\n",
    "    \"complete\": df_driver_index,\n",
    "    \"sample\" : df_driver_index_sample,\n",
    "    \"dummy\": df_driver_index_sample\n",
    "}\n",
    "\n",
    "# Driver Split Dictionary\n",
    "dict_driver_split = {\n",
    "    \"train\": df_images_train,\n",
    "    \"validation\" : df_images_val,\n",
    "    \"dummy\": df_images_val\n",
    "}\n",
    "\n",
    "dict_overall_copy = {\n",
    "    \"split_random\" : dict_random_split,\n",
    "    \"split_driver\" : dict_driver_split\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering Initial For Loop\n",
      "\n",
      "Entering If Statment - Recursive\n",
      "\n",
      "Entering Initial For Loop\n",
      "\n",
      "Entering Else Statment\n",
      "Folder: split_random\n",
      "Key: complete\n",
      "\n",
      "Entering Initial For Loop\n",
      "\n",
      "Entering Else Statment\n",
      "Folder: split_random\n",
      "Key: sample\n",
      "\n",
      "Entering Initial For Loop\n",
      "\n",
      "Entering Else Statment\n",
      "Folder: split_random\n",
      "Key: dummy\n",
      "\n",
      "Entering Initial For Loop\n",
      "\n",
      "Entering If Statment - Recursive\n",
      "\n",
      "Entering Initial For Loop\n",
      "\n",
      "Entering Else Statment\n",
      "Folder: split_driver\n",
      "Key: train\n",
      "\n",
      "Entering Initial For Loop\n",
      "\n",
      "Entering Else Statment\n",
      "Folder: split_driver\n",
      "Key: validation\n",
      "\n",
      "Entering Initial For Loop\n",
      "\n",
      "Entering Else Statment\n",
      "Folder: split_driver\n",
      "Key: dummy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "copy_imgs(dict_overall_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Folder Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now verify the size of the resulting folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calculates the number of files in a directory, recursively\n",
    "def num_files(nested_dict, folder = \"default\"):\n",
    "    \n",
    "     for key, value in nested_dict.items():\n",
    "        #print(\"Entering Initial For Loop\")\n",
    "        #print(\"\")\n",
    "        if type(value) is dict:\n",
    "            print(\"Entering If Statment - Recursive\")\n",
    "            print(\"\")\n",
    "            # Define Local Values\n",
    "            folder = key\n",
    "            num_files(value, folder = folder)\n",
    "           \n",
    "        else:\n",
    "            \n",
    "            path_current = \"D:\\\\Notebooks\\\\dsba_6190\\\\team_project\\\\image_classification_preprocessing\"\n",
    "            prefix_folder = \"imgs\\\\train-\" + folder\n",
    "            prefix_subfolder = key\n",
    "            dir_check = os.path.join(path_current, prefix_folder, prefix_subfolder)\n",
    "            \n",
    "            cpt = sum([len(files) for r, d, files in os.walk(dir_check)])\n",
    "            print(\"Directory Analyzed:\\t{}\".format(dir_check))\n",
    "            print(\"\\t\\t\\tThere are \" + \"\\033[1m\" + \"{}\".format(cpt) + \"\\033[0m\" + \" image files in the \" + \n",
    "                  \"\\033[1m\"+ folder + \"\\\\\" + prefix_subfolder +  \"\\033[0m\" + \" directory.\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering If Statment - Recursive\n",
      "\n",
      "Directory Analyzed:\tD:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\imgs\\train-split_random\\complete\n",
      "\t\t\tThere are \u001b[1m22424\u001b[0m image files in the \u001b[1msplit_random\\complete\u001b[0m directory.\n",
      "\n",
      "Directory Analyzed:\tD:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\imgs\\train-split_random\\sample\n",
      "\t\t\tThere are \u001b[1m2243\u001b[0m image files in the \u001b[1msplit_random\\sample\u001b[0m directory.\n",
      "\n",
      "Directory Analyzed:\tD:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\imgs\\train-split_random\\dummy\n",
      "\t\t\tThere are \u001b[1m2243\u001b[0m image files in the \u001b[1msplit_random\\dummy\u001b[0m directory.\n",
      "\n",
      "Entering If Statment - Recursive\n",
      "\n",
      "Directory Analyzed:\tD:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\imgs\\train-split_driver\\train\n",
      "\t\t\tThere are \u001b[1m17819\u001b[0m image files in the \u001b[1msplit_driver\\train\u001b[0m directory.\n",
      "\n",
      "Directory Analyzed:\tD:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\imgs\\train-split_driver\\validation\n",
      "\t\t\tThere are \u001b[1m4605\u001b[0m image files in the \u001b[1msplit_driver\\validation\u001b[0m directory.\n",
      "\n",
      "Directory Analyzed:\tD:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\imgs\\train-split_driver\\dummy\n",
      "\t\t\tThere are \u001b[1m4605\u001b[0m image files in the \u001b[1msplit_driver\\dummy\u001b[0m directory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_files(dict_overall_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Flow\n",
    "\n",
    "Some of the downstream processing steps are computationally heavy. Therefore, I'm going to put in some guardrails in order to avoid unecessarily re-running these code chunks\n",
    "\n",
    "To check if any of the input files have changed, which would then require reprocessing the images, we will use Hash checksum values to document the state of the input folders. We will check against this Hash state to determine if we need to reprocess the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## im2rec Function Calls\n",
    "The following functions use the im2rec.py tool to process the image inputs.\n",
    "\n",
    "[https://mxnet.apache.org/api/faq/recordio](https://mxnet.apache.org/api/faq/recordio) (Accessed on 3/20/2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LST File Generation\n",
    "This function performs the first step, creating LST mapping files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2rec_lst(folder_plus_train, folder, key):\n",
    "    \n",
    "    train_ratio = \"\"\n",
    "    lst_file_path = os.path.join(\"model_input_files\", folder, key, key)\n",
    "    img_loc =  os.path.join(\"imgs\", folder_plus_train, key)\n",
    "    \n",
    "    # Verify LST File PAth Exists. If not, create.\n",
    "    os.makedirs(os.path.dirname(lst_file_path), exist_ok=True)\n",
    "    \n",
    "    #Print Statements\n",
    "    print(\"Split Method: {}\".format(folder_plus_train))\n",
    "    print(\"Database: {}\".format(key))\n",
    "    print(\"LST File Path: {}\".format(lst_file_path))\n",
    "    \n",
    "    if \"random\" in folder:\n",
    "        # Use im2rec to create train/val data split\n",
    "        print(\"Random dectected in folder string.\")\n",
    "        %run incubator-mxnet/tools/im2rec.py --list --recursive --train-ratio 0.8 {lst_file_path} {img_loc}\n",
    "        \n",
    "    else:\n",
    "        # Train/val data split already defined\n",
    "        %run incubator-mxnet/tools/im2rec.py --list --recursive {lst_file_path} {img_loc}\n",
    "    \n",
    "    return img_loc, lst_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recordio Conversion\n",
    "This function converts the images into a RECORDIO binary file format. In addition to converting images from raw image files to RECORDIO, we need to resize the images. In the Amazon Sagemaker Image Classification Hyperparameters documentation ([here](https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html)) there is a parameter called **image_shape**. This parameter requires a string with three numbers, comma seperated (i.e. \"1,2,3\").\n",
    "\n",
    "The first value is **num_channels**. This is the number of channels our input images have. They are color images, so they have three channels (Red, Green, and Blue aka RGB). The second and third values are the heigth and width of the images, in pixels. Technically, the algorithm can accomadate images of any size, but if the images are too large, there may be memory constraints. It indicates typical dimensions are 244 x 244. Our images come are 640 x 480. By area this is 5x bigger. So, we need to resize the images.\n",
    "\n",
    "The tool **im2rec.py** is capable of resizing images during the RECORDIO conversion. All we need to to is add the argument \"**--resize #**\" to the command line entry. The number in the command line argument is what the tool will resize the ***shortest edge*** of the input image. The larger edge will be resized to maintain the same relative dimensions. \n",
    "\n",
    "Therefore, we are going to resize the input images so that the resulting area is approximately equal to the area of a 244 x 244 image. The dimensions we are going to use is 210 x 280. This maintains the relative size of the input images, while have an area 98.7% that of the default image size. So, in the command line argument, the call will be **--resize 210**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2rec_rec(img_loc, lst_file_path):\n",
    "    \n",
    "    if \"random\" in lst_file_path:\n",
    "        lst_file_path_train = lst_file_path + \"_train\"\n",
    "        lst_file_path_val = lst_file_path + \"_val\"\n",
    "       \n",
    "        # Train\n",
    "        lst_file_path = lst_file_path_train\n",
    "        print()\n",
    "        print(\"Creating REC file for Path: {}\".format(lst_file_path))\n",
    "        %run incubator-mxnet/tools/im2rec.py  --resize 210 {lst_file_path} {img_loc} \n",
    "        \n",
    "        # Validation\n",
    "        print()\n",
    "        lst_file_path = lst_file_path_val\n",
    "        print(\"Creating REC file for Path: {}\".format(lst_file_path))\n",
    "        %run incubator-mxnet/tools/im2rec.py  --resize 210 {lst_file_path} {img_loc} \n",
    "    \n",
    "    else:\n",
    "        print()\n",
    "        print(\"Creating REC file for Path: {}\".format(lst_file_path))\n",
    "        %run incubator-mxnet/tools/im2rec.py  --resize 210 {lst_file_path} {img_loc} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Hash Checksum\n",
    "The following function generates the Hash of a directory (training or validation images) and writes the Hash in a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_hash(folder_plus_train, key, file_path, file_dir):\n",
    "\n",
    "    # Generate Hash\n",
    "    dir_hash = dirhash(file_dir, 'sha256')\n",
    "    \n",
    "    # Write to CSV\n",
    "    dict_dir_hash = {'hash': [dir_hash]}\n",
    "    df_dir_hash = pd.DataFrame(dict_dir_hash)\n",
    "    df_dir_hash.to_csv(file_path, index=False)\n",
    "    print(\"Hash for {} images in {} successfully written to CSV.\".format(key, folder_plus_train))\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Processing and Writing Hash\n",
    "Whenever we process the input data we need to write a new hash, and vice versa. Therefore, for simplicity in the final code, we will lump these actions together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2rec_and_write(folder_plus_train, folder, key, file_name, file_path_hash, file_dir):\n",
    "        print()\n",
    "        print(\"Entering LST Creation\")\n",
    "        print()\n",
    "        img_loc, lst_file_path = im2rec_lst(folder_plus_train, folder, key)\n",
    "        print(\"LST Creation Complete\")\n",
    "        print()\n",
    "        \n",
    "        print(\"Entering REC Creation\")\n",
    "        print()\n",
    "        im2rec_rec(img_loc, lst_file_path)\n",
    "        print(\"REC Creation Complete\")\n",
    "        print()\n",
    "        \n",
    "        write_hash(folder_plus_train, key, file_path_hash, file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Flow Function\n",
    "The following function will verify the hash of the current files matches the older record. If they match, no action is taken. If they do not match, the input data is processed, and a new hash is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_convert(nested_dict, folder = \"default\"):\n",
    "    \n",
    "    for key, value in nested_dict.items():\n",
    "        #print(\"Entering Initial For Loop\")\n",
    "        #print(\"\")\n",
    "        if type(value) is dict:\n",
    "            print(\"Entering If Statment - Recursive\")\n",
    "            print(\"\")\n",
    "            # Define Local Values\n",
    "            folder = key\n",
    "            image_convert(value, folder = folder)\n",
    "           \n",
    "        else:\n",
    "            print(\"Entering Else Statement - Execute\")\n",
    "    \n",
    "            print(\"Split Method: {}\".format(folder))\n",
    "            print(\"Dataset: {}\".format(key))\n",
    "            print()\n",
    "\n",
    "\n",
    "            # Generate File Name, Path and Directory for Hash\n",
    "            folder_plus_train = \"train-\" + folder\n",
    "            file_name = \"hash_{}.csv\".format(key)\n",
    "            file_path_hash = os.path.join(\"hash\", folder, file_name)\n",
    "            file_dir = os.path.join('imgs', folder_plus_train, key)\n",
    "            \n",
    "            # Verify Hash File Location Exists. If not, create\n",
    "            os.makedirs(os.path.dirname(file_path_hash), exist_ok=True)\n",
    "\n",
    "            # Print for Sanity Check\n",
    "            print(\"Hash File Name: {}\".format(file_name))\n",
    "            print(\"Hash File Path: {}\".format(file_path_hash))\n",
    "            print(\"Image Set File Directory: {}\".format(file_dir))\n",
    "            print()\n",
    "\n",
    "            # Generate Current Hash\n",
    "            print('New Hash - Generating')\n",
    "            hash_new = dirhash(file_dir, 'sha256')\n",
    "            print('New Hash - Generated')\n",
    "            print()\n",
    "\n",
    "            if not os.path.exists(file_path_hash):\n",
    "                print(\"Hash for {} images in {} does not exist.\".format(key, folder))\n",
    "                print()\n",
    "                im2rec_and_write(folder_plus_train, folder, key, \n",
    "                                 file_name, file_path_hash, file_dir)\n",
    "\n",
    "            else:\n",
    "                # Read Existing Hash\n",
    "                df_hash_old = pd.read_csv(file_path_hash)\n",
    "                hash_old = df_hash_old.iloc[0]['hash']\n",
    "\n",
    "                if hash_old == hash_new:\n",
    "                    print(\"Hash for {} images in {} are equal.\".format(key, folder))\n",
    "                    print(\"Hash not replaced.\")\n",
    "                    print(\"Processing not required.\")\n",
    "                    print()\n",
    "\n",
    "                else:\n",
    "                    print(\"Hash for {} images in {} are NOT equal.\".format(key, folder))\n",
    "                    print()\n",
    "                    im2rec_and_write(folder_plus_train, folder, key, \n",
    "                                     file_name, file_path_hash, file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Process\n",
    "Runnign the following code chuck will execute the process flow function develpoed above for both training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering If Statment - Recursive\n",
      "\n",
      "Entering Else Statement - Execute\n",
      "Split Method: split_random\n",
      "Dataset: complete\n",
      "\n",
      "Hash File Name: hash_complete.csv\n",
      "Hash File Path: hash\\split_random\\hash_complete.csv\n",
      "Image Set File Directory: imgs\\train-split_random\\complete\n",
      "\n",
      "New Hash - Generating\n",
      "New Hash - Generated\n",
      "\n",
      "Hash for complete images in split_random are equal.\n",
      "Hash not replaced.\n",
      "Processing not required.\n",
      "\n",
      "Entering Else Statement - Execute\n",
      "Split Method: split_random\n",
      "Dataset: sample\n",
      "\n",
      "Hash File Name: hash_sample.csv\n",
      "Hash File Path: hash\\split_random\\hash_sample.csv\n",
      "Image Set File Directory: imgs\\train-split_random\\sample\n",
      "\n",
      "New Hash - Generating\n",
      "New Hash - Generated\n",
      "\n",
      "Hash for sample images in split_random are equal.\n",
      "Hash not replaced.\n",
      "Processing not required.\n",
      "\n",
      "Entering Else Statement - Execute\n",
      "Split Method: split_random\n",
      "Dataset: dummy\n",
      "\n",
      "Hash File Name: hash_dummy.csv\n",
      "Hash File Path: hash\\split_random\\hash_dummy.csv\n",
      "Image Set File Directory: imgs\\train-split_random\\dummy\n",
      "\n",
      "New Hash - Generating\n",
      "New Hash - Generated\n",
      "\n",
      "Hash for dummy images in split_random are equal.\n",
      "Hash not replaced.\n",
      "Processing not required.\n",
      "\n",
      "Entering If Statment - Recursive\n",
      "\n",
      "Entering Else Statement - Execute\n",
      "Split Method: split_driver\n",
      "Dataset: train\n",
      "\n",
      "Hash File Name: hash_train.csv\n",
      "Hash File Path: hash\\split_driver\\hash_train.csv\n",
      "Image Set File Directory: imgs\\train-split_driver\\train\n",
      "\n",
      "New Hash - Generating\n",
      "New Hash - Generated\n",
      "\n",
      "Hash for train images in split_driver are equal.\n",
      "Hash not replaced.\n",
      "Processing not required.\n",
      "\n",
      "Entering Else Statement - Execute\n",
      "Split Method: split_driver\n",
      "Dataset: validation\n",
      "\n",
      "Hash File Name: hash_validation.csv\n",
      "Hash File Path: hash\\split_driver\\hash_validation.csv\n",
      "Image Set File Directory: imgs\\train-split_driver\\validation\n",
      "\n",
      "New Hash - Generating\n",
      "New Hash - Generated\n",
      "\n",
      "Hash for validation images in split_driver are equal.\n",
      "Hash not replaced.\n",
      "Processing not required.\n",
      "\n",
      "Entering Else Statement - Execute\n",
      "Split Method: split_driver\n",
      "Dataset: dummy\n",
      "\n",
      "Hash File Name: hash_dummy.csv\n",
      "Hash File Path: hash\\split_driver\\hash_dummy.csv\n",
      "Image Set File Directory: imgs\\train-split_driver\\dummy\n",
      "\n",
      "New Hash - Generating\n",
      "New Hash - Generated\n",
      "\n",
      "Hash for dummy images in split_driver are equal.\n",
      "Hash not replaced.\n",
      "Processing not required.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_convert(dict_overall_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish AWS Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = 'dsba_6190_proj_4'\n",
    "region = 'us-east-1'\n",
    "bucket = 'dsba-6190-final-team-project'\n",
    "prefix = \"channels_rec\"\n",
    "split_method = \"split_driver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session(profile_name = profile,\n",
    "                               region_name = region)\n",
    "\n",
    "s3_resource = session.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload\n",
    "We now upload the Recordio format files to S3. The Amazon Sagemaker Algorithm requires a specific folder format, with the training and validation data as seperate subfolders of the same directory, with the names train and validation, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(file_name_local, s3_key): \n",
    "    s3_resource.Bucket(bucket).upload_file(Filename = file_name_local, Key = s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"model_input_files\" \n",
    "prefix_s3_1 = \" channels\"\n",
    "prefix_s3_2 = \"rec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split_Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_s3_random(status, dataset):\n",
    "    # Variables\n",
    "    split_type = \"split_random\"\n",
    "    status_tag = status\n",
    "    \n",
    "    if \"al\" in status:\n",
    "        status_tag = \"val\"\n",
    "    \n",
    "    file_name_local = dataset + \"_\" + status_tag +\".rec\"  \n",
    "    file_name_s3 = status + \".rec\"\n",
    "    file_for_upload = os.path.join(folder, split_type, dataset, file_name_local).replace('\\\\','/')\n",
    "    s3_key = os.path.join(prefix_s3_1, prefix_s3_2, split_type, dataset, status, file_name_s3).replace('\\\\','/')\n",
    "    upload_to_s3(file_for_upload, s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"complete\"\n",
    "\n",
    "#Training Set\n",
    "#upload_s3_random(status = \"train\", dataset = dataset)\n",
    "\n",
    "# Validation Set\n",
    "#upload_s3_random(status = \"validation\", dataset = dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"sample\"\n",
    "\n",
    "# Training Set\n",
    "#upload_s3_random(status = \"train\", dataset = dataset)\n",
    "\n",
    "# Validation Set\n",
    "#upload_s3_random(status = \"validation\", dataset = dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split_Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_s3_driver(status, dataset = \"complete\"):\n",
    "    # Variables\n",
    "    split_type = \"split_driver\"\n",
    "    status_tag = status\n",
    "   \n",
    "    file_name_local = status + \".rec\"  \n",
    "    file_name_s3 = file_name_local\n",
    "    file_for_upload = os.path.join(folder, split_type, status, file_name_local).replace('\\\\','/')\n",
    "    s3_key = os.path.join(prefix_s3_1, prefix_s3_2, split_type, dataset, status, file_name_s3).replace('\\\\','/')\n",
    "    upload_to_s3(file_for_upload, s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"complete\"\n",
    "\n",
    "# Training Set\n",
    "#upload_s3_driver(status = \"train\")\n",
    "\n",
    "# Validation Set\n",
    "#upload_s3_driver(status = \"validation\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "State Farm Distracted Driver Process Flow.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
