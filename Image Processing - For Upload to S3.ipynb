{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "## Project Folder Structure\n",
    "The data for this file is too large to store with the github repo. Therefore, the files need to be added manually. The raw image files can be found here:\n",
    "\n",
    "[https://www.kaggle.com/c/state-farm-distracted-driver-detection/data](https://www.kaggle.com/c/state-farm-distracted-driver-detection/data) (Accessed on 3/21/2020)\n",
    "\n",
    "The file structre should be as follows:\n",
    "\n",
    "--Working\n",
    "\n",
    "----imgs\n",
    "\n",
    "------complete\n",
    "  \n",
    "--------train\n",
    "\n",
    "--------test\n",
    "\n",
    "The train and test folders come directly from the kaggle dataset. Subsequent folders will be generated by this notebook.\n",
    "\n",
    "## Scope\n",
    "This notebook is to prepare the images in the distracted driver dataset locally, for upload to S3.\n",
    "\n",
    "This notebook will create two versions of the data. The first will be a LST mapping file, with the image files in JPEG format. The second will be a recordio format.\n",
    "\n",
    "A Complete dataset and Sample dataset will be created and uploaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: checksumdir in c:\\users\\canfi\\anaconda2\\envs\\dsba_6190_team_proj\\lib\\site-packages (1.1.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install checksumdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1z9yTpGbACbX"
   },
   "source": [
    "## Library / Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhXJ8brV_4Qb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random \n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "\n",
    "import hashlib\n",
    "from checksumdir import dirhash\n",
    "\n",
    "from filecmp import dircmp\n",
    "\n",
    "#Settings\n",
    "seed = 5590"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "406FN849AG3C"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RpUao_lNAE88"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/DSBA-6190-Final-Project-Team/DSBA-6190_Final-Project/master/wine_predict/data/driver_imgs_list.csv\"\n",
    "path_file = 'data/driver_imgs_list.csv'\n",
    "\n",
    "df_driver_index = pd.read_csv(path_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xr04mwBdAwNY"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1W_1IwhLA7vT",
    "outputId": "6c263b25-d00d-46cc-c0cc-35a5f5e9f47c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22424, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_driver_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "n3glHqjBA3wI",
    "outputId": "d57b86d9-fe7b-4640-f241-3cba70bfab9e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_44733.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_72999.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_25094.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_69092.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_92629.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject classname            img\n",
       "0    p002        c0  img_44733.jpg\n",
       "1    p002        c0  img_72999.jpg\n",
       "2    p002        c0  img_25094.jpg\n",
       "3    p002        c0  img_69092.jpg\n",
       "4    p002        c0  img_92629.jpg"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_driver_index.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R-sfjF4eAyMI",
    "outputId": "2a8fafe6-343c-4cc7-8b1c-5f64856d2518"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'classname', 'img'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_driver_index.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2w7VZnGBz8G"
   },
   "source": [
    "The following lists each unique driver along with the number of different classname and images are associated with each. Note the number of classnames is not unique, so the images an classnames have equal frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 886
    },
    "colab_type": "code",
    "id": "_vqngHEZBEVL",
    "outputId": "f391960d-5245-42b3-ba56-0cde324c5a5b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p002</th>\n",
       "      <td>725</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p012</th>\n",
       "      <td>823</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p014</th>\n",
       "      <td>876</td>\n",
       "      <td>876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p015</th>\n",
       "      <td>875</td>\n",
       "      <td>875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p016</th>\n",
       "      <td>1078</td>\n",
       "      <td>1078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p021</th>\n",
       "      <td>1237</td>\n",
       "      <td>1237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p022</th>\n",
       "      <td>1233</td>\n",
       "      <td>1233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p024</th>\n",
       "      <td>1226</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p026</th>\n",
       "      <td>1196</td>\n",
       "      <td>1196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p035</th>\n",
       "      <td>848</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p039</th>\n",
       "      <td>651</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p041</th>\n",
       "      <td>605</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p042</th>\n",
       "      <td>591</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p045</th>\n",
       "      <td>724</td>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p047</th>\n",
       "      <td>835</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p049</th>\n",
       "      <td>1011</td>\n",
       "      <td>1011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p050</th>\n",
       "      <td>790</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p051</th>\n",
       "      <td>920</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p052</th>\n",
       "      <td>740</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p056</th>\n",
       "      <td>794</td>\n",
       "      <td>794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p061</th>\n",
       "      <td>809</td>\n",
       "      <td>809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p064</th>\n",
       "      <td>820</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p066</th>\n",
       "      <td>1034</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p072</th>\n",
       "      <td>346</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p075</th>\n",
       "      <td>814</td>\n",
       "      <td>814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p081</th>\n",
       "      <td>823</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         classname   img\n",
       "subject                 \n",
       "p002           725   725\n",
       "p012           823   823\n",
       "p014           876   876\n",
       "p015           875   875\n",
       "p016          1078  1078\n",
       "p021          1237  1237\n",
       "p022          1233  1233\n",
       "p024          1226  1226\n",
       "p026          1196  1196\n",
       "p035           848   848\n",
       "p039           651   651\n",
       "p041           605   605\n",
       "p042           591   591\n",
       "p045           724   724\n",
       "p047           835   835\n",
       "p049          1011  1011\n",
       "p050           790   790\n",
       "p051           920   920\n",
       "p052           740   740\n",
       "p056           794   794\n",
       "p061           809   809\n",
       "p064           820   820\n",
       "p066          1034  1034\n",
       "p072           346   346\n",
       "p075           814   814\n",
       "p081           823   823"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drivers_gb = df_driver_index.groupby(['subject'])\n",
    "drivers_gb.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0KTpe42Cqi6"
   },
   "source": [
    "We will set the training/validation split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SNDV8uWICp9y"
   },
   "outputs": [],
   "source": [
    "train_val_split = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6gBnk_kDXtt"
   },
   "source": [
    "To split the data into training and validation sets, we'll need a unique index of drivers. We don't need the frequency counts.\n",
    "\n",
    "We will create a shuffled list of unique drivers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "5Lps82L_Dn9-",
    "outputId": "f0aff480-dc8e-471a-d45f-ee9fd2f8d09e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p049', 'p022', 'p012', 'p075', 'p045', 'p072', 'p026', 'p042', 'p015', 'p039', 'p047', 'p081', 'p052', 'p014', 'p021', 'p035', 'p061', 'p064', 'p066', 'p056', 'p002', 'p041', 'p024', 'p016', 'p051', 'p050']\n"
     ]
    }
   ],
   "source": [
    "drivers_unique = drivers_gb.groups.keys()\n",
    "drivers_unique = list(drivers_unique)\n",
    "\n",
    "random.Random(seed).shuffle(drivers_unique)\n",
    "print(drivers_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YOClOK2lEZsD"
   },
   "source": [
    "We'll set the list of drivers in the train set and the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rzmgBKnHEfmx"
   },
   "outputs": [],
   "source": [
    "num_drivers_val = round(len(drivers_unique)*train_val_split)\n",
    "#print(num_drivers_val)\n",
    "num_drivers_train = len(drivers_unique) - num_drivers_val\n",
    "#print(num_drivers_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "keIIvYaDFeGv",
    "outputId": "a0445d0d-d0e2-4780-a6ab-8ae3abce0bca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p049', 'p022', 'p012', 'p075', 'p045', 'p072', 'p026', 'p042']\n"
     ]
    }
   ],
   "source": [
    "drivers_val = drivers_unique[:num_drivers_val]\n",
    "print(drivers_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aA1httPIFte_",
    "outputId": "e23ed327-f12f-408b-bf00-8cfeb1d49f4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p015', 'p039', 'p047', 'p081', 'p052', 'p014', 'p021', 'p035', 'p061', 'p064', 'p066', 'p056', 'p002', 'p041', 'p024', 'p016', 'p051', 'p050']\n"
     ]
    }
   ],
   "source": [
    "drivers_train = drivers_unique[-num_drivers_train:]\n",
    "print(drivers_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hNA4cd_cPquX"
   },
   "source": [
    "# Training and Validation Image Lists\n",
    "We'll now create two lists, one list of every image file name associated with the trainging set, another list associated with the validation set. \n",
    "\n",
    "We will use these lists to filter the overall lst mapping file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f8a4uhBKP-uQ"
   },
   "outputs": [],
   "source": [
    "df_images_val = df_driver_index[df_driver_index['subject'].isin(drivers_val)]\n",
    "df_images_train = df_driver_index[~df_driver_index['subject'].isin(drivers_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "vHXC2dsXQ2Po",
    "outputId": "d2f863d9-7bdb-493f-d31e-148314e19378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15686, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_44733.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_72999.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_25094.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_69092.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_92629.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject classname            img\n",
       "0    p002        c0  img_44733.jpg\n",
       "1    p002        c0  img_72999.jpg\n",
       "2    p002        c0  img_25094.jpg\n",
       "3    p002        c0  img_69092.jpg\n",
       "4    p002        c0  img_92629.jpg"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_images_train.shape)\n",
    "df_images_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "ex0G4NQCRn0j",
    "outputId": "cc7473cf-2acc-41b2-aa75-507a59349d7c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6738, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_10206.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_27079.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_50749.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_97089.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_37741.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject classname            img\n",
       "725    p012        c0  img_10206.jpg\n",
       "726    p012        c0  img_27079.jpg\n",
       "727    p012        c0  img_50749.jpg\n",
       "728    p012        c0  img_97089.jpg\n",
       "729    p012        c0  img_37741.jpg"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_images_val.shape)\n",
    "df_images_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move Images\n",
    "We need to create two folders images, one for training, and one for validation. We will leave the complete folder of images alone, in case we need to go back and extract information from that folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Copy\n",
    "The following function takes the training or validaiton dataframe and iterates through each row. It then copies each file from the overall data to the respective folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_imgs_all(status, df):\n",
    "    prefix_overall = \"imgs\\\\complete\\\\train\"\n",
    "    folder_suffix = \"_subset\"\n",
    "    folder_name = status + folder_suffix\n",
    "    prefix_status = os.path.join(\"imgs\",folder_name)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        path_src = os.path.join(prefix_overall,\n",
    "                                row['classname'], \n",
    "                                row['img']).replace('\\\\','/')\n",
    "             \n",
    "       \n",
    "        path_dst = os.path.join(\"imgs\\\\complete\",\n",
    "                               folder_name, \n",
    "                               row['classname'],\n",
    "                               row['img']).replace('\\\\','/')\n",
    "        \n",
    "        #print(\"Source File:\\t\\t{}\".format(path_src))\n",
    "        #print(\"Destination File:\\t{}\".format(path_dst))\n",
    "    \n",
    "        \n",
    "        if not os.path.exists(path_dst):\n",
    "            # Verify Directory Exists. Create if Not \n",
    "            os.makedirs(os.path.dirname(path_dst), exist_ok=True)\n",
    "            \n",
    "            shutil.copy(path_src, path_dst)\n",
    "            #print(\"Loop Start\")\n",
    "            #print(\"Source File:\\t\\t{}\".format(path_src))\n",
    "            #print(\"Destination File:\\t{}\".format(path_dst))\n",
    "            #print()\n",
    "        #else:\n",
    "            #print(\"File Exists. No Copy Made.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a dictionary containing the status and associated dataframes. This will be looped over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_image_df = {\n",
    "    \"train\": df_images_train,\n",
    "    \"validation\" : df_images_val\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_image_df:\n",
    "    copy_imgs_all(key, dict_image_df[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "In order to test the image classification algorithm, we are going to create a subsample of the training and validation test sets. We will need to create a list of images for each, and then copy the images into their own directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classname</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">c0</th>\n",
       "      <th>6903</th>\n",
       "      <td>p024</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_86347.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4449</th>\n",
       "      <td>p021</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_69997.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9311</th>\n",
       "      <td>p035</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_30218.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17010</th>\n",
       "      <td>p056</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_17591.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15491</th>\n",
       "      <td>p051</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_24357.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">c9</th>\n",
       "      <th>4273</th>\n",
       "      <td>p016</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_43974.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13457</th>\n",
       "      <td>p047</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_55331.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>p014</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_36650.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13507</th>\n",
       "      <td>p047</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_44150.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18519</th>\n",
       "      <td>p061</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_81214.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                subject classname            img\n",
       "classname                                       \n",
       "c0        6903     p024        c0  img_86347.jpg\n",
       "          4449     p021        c0  img_69997.jpg\n",
       "          9311     p035        c0  img_30218.jpg\n",
       "          17010    p056        c0  img_17591.jpg\n",
       "          15491    p051        c0  img_24357.jpg\n",
       "...                 ...       ...            ...\n",
       "c9        4273     p016        c9  img_43974.jpg\n",
       "          13457    p047        c9  img_55331.jpg\n",
       "          2386     p014        c9  img_36650.jpg\n",
       "          13507    p047        c9  img_44150.jpg\n",
       "          18519    p061        c9  img_81214.jpg\n",
       "\n",
       "[1567 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_images_train_sample = df_images_train.groupby('classname').apply(pd.DataFrame.sample, \n",
    "                                                                    frac = sample_rate, \n",
    "                                                                    random_state = seed)\n",
    "df_images_train_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classname</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">c0</th>\n",
       "      <th>8202</th>\n",
       "      <td>p026</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_34269.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11400</th>\n",
       "      <td>p042</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_59032.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13573</th>\n",
       "      <td>p049</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_11474.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8177</th>\n",
       "      <td>p026</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_26025.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5662</th>\n",
       "      <td>p022</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_9678.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">c9</th>\n",
       "      <th>11937</th>\n",
       "      <td>p042</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_20372.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12668</th>\n",
       "      <td>p045</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_75520.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14464</th>\n",
       "      <td>p049</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_30754.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>p012</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_74547.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14420</th>\n",
       "      <td>p049</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_44355.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>673 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                subject classname            img\n",
       "classname                                       \n",
       "c0        8202     p026        c0  img_34269.jpg\n",
       "          11400    p042        c0  img_59032.jpg\n",
       "          13573    p049        c0  img_11474.jpg\n",
       "          8177     p026        c0  img_26025.jpg\n",
       "          5662     p022        c0   img_9678.jpg\n",
       "...                 ...       ...            ...\n",
       "c9        11937    p042        c9  img_20372.jpg\n",
       "          12668    p045        c9  img_75520.jpg\n",
       "          14464    p049        c9  img_30754.jpg\n",
       "          1495     p012        c9  img_74547.jpg\n",
       "          14420    p049        c9  img_44355.jpg\n",
       "\n",
       "[673 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_images_val_sample = df_images_val.groupby('classname').apply(pd.DataFrame.sample, \n",
    "                                                                frac = sample_rate, \n",
    "                                                                random_state = seed)\n",
    "df_images_val_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_imgs_sample(status, df):\n",
    "    \n",
    "    subfolder_complete = \"complete\"\n",
    "    subfolder_sample = \"sample\"\n",
    "    folder_name = status + \"_subset\"\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        path_src = os.path.join(\"imgs\",\n",
    "                                subfolder_complete,\n",
    "                                folder_name,\n",
    "                                row['classname'], \n",
    "                                row['img']).replace('\\\\','/')\n",
    "                \n",
    "        path_dst = os.path.join(\"imgs\",\n",
    "                                subfolder_sample,\n",
    "                                folder_name,\n",
    "                                row['classname'],\n",
    "                                row['img']).replace('\\\\','/')\n",
    "        \n",
    "        #print(\"Loop Start\")\n",
    "        #print(\"Source File:\\t\\t{}\".format(path_src))\n",
    "        #print(\"Destination File:\\t{}\".format(path_dst))\n",
    "        #print(\"\")    \n",
    "        \n",
    "        # verify\n",
    "        os.makedirs(os.path.dirname(path_dst), exist_ok=True)\n",
    "        \n",
    "        if not os.path.exists(path_dst):\n",
    "            shutil.copy(path_src, path_dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We establish a dictionary of the sample image lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train':                 subject classname            img\n",
       " classname                                       \n",
       " c0        6903     p024        c0  img_86347.jpg\n",
       "           4449     p021        c0  img_69997.jpg\n",
       "           9311     p035        c0  img_30218.jpg\n",
       "           17010    p056        c0  img_17591.jpg\n",
       "           15491    p051        c0  img_24357.jpg\n",
       " ...                 ...       ...            ...\n",
       " c9        4273     p016        c9  img_43974.jpg\n",
       "           13457    p047        c9  img_55331.jpg\n",
       "           2386     p014        c9  img_36650.jpg\n",
       "           13507    p047        c9  img_44150.jpg\n",
       "           18519    p061        c9  img_81214.jpg\n",
       " \n",
       " [1567 rows x 3 columns],\n",
       " 'validation':                 subject classname            img\n",
       " classname                                       \n",
       " c0        8202     p026        c0  img_34269.jpg\n",
       "           11400    p042        c0  img_59032.jpg\n",
       "           13573    p049        c0  img_11474.jpg\n",
       "           8177     p026        c0  img_26025.jpg\n",
       "           5662     p022        c0   img_9678.jpg\n",
       " ...                 ...       ...            ...\n",
       " c9        11937    p042        c9  img_20372.jpg\n",
       "           12668    p045        c9  img_75520.jpg\n",
       "           14464    p049        c9  img_30754.jpg\n",
       "           1495     p012        c9  img_74547.jpg\n",
       "           14420    p049        c9  img_44355.jpg\n",
       " \n",
       " [673 rows x 3 columns]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_image_df_sample = {\n",
    "    \"train\": df_images_train_sample,\n",
    "    \"validation\" : df_images_val_sample\n",
    "}\n",
    "\n",
    "dict_image_df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_image_df_sample:\n",
    "    copy_imgs_sample(key, dict_image_df_sample[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Folder Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now verify the size of the resulting folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calculates the number of files in a directory, recursively\n",
    "def num_files(dir_scan):\n",
    "    cpt = sum([len(files) for r, d, files in os.walk(dir_scan)])\n",
    "    print(\"Directory Analyzed:\\t{}\".format(dir_scan))\n",
    "    print(\"\\t\\t\\tThere are {} image files in the directory.\".format(cpt))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_current = \"D:\\\\Notebooks\\\\dsba_6190\\\\team_project\\\\image_classification_preprocessing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Analyzed:\tD:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\imgs\\complete\\train_subset\n",
      "\t\t\tThere are 15686 image files in the directory.\n",
      "\n",
      "Directory Analyzed:\tD:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\imgs\\complete\\validation_subset\n",
      "\t\t\tThere are 6738 image files in the directory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prefix_training = \"imgs\\\\complete\\\\train_subset\"\n",
    "prefix_validation = \"imgs\\\\complete\\\\validation_subset\"\n",
    "\n",
    "# Training\n",
    "dir_train = os.path.join(path_current, prefix_training)\n",
    "num_files(dir_train)\n",
    "\n",
    "# Validation\n",
    "dir_validation = os.path.join(path_current, prefix_validation)\n",
    "num_files(dir_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\imgs\\sample\\train_subset\n",
      "Directory Analyzed:\tD:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\imgs\\sample\\train_subset\n",
      "\t\t\tThere are 1567 image files in the directory.\n",
      "\n",
      "Directory Analyzed:\tD:\\Notebooks\\dsba_6190\\team_project\\image_classification_preprocessing\\imgs\\sample\\validation_subset\n",
      "\t\t\tThere are 673 image files in the directory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prefix_training = \"imgs\\\\sample\\\\train_subset\"\n",
    "prefix_validation = \"imgs\\\\sample\\\\validation_subset\"\n",
    "\n",
    "# Training\n",
    "dir_train = os.path.join(path_current, prefix_training)\n",
    "print(dir_train)\n",
    "num_files(dir_train)\n",
    "\n",
    "# Validation\n",
    "dir_validation = os.path.join(path_current, prefix_validation)\n",
    "num_files(dir_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Flow\n",
    "\n",
    "Some of the downstream processing steps are computationally heavy. Therefore, in order to avoid running these code chunks, without manually commenting them out once run. \n",
    "\n",
    "To check if any of the input files have changed, which would then require reprocessing the images, we will use Hash checksum values to document the state of the inputs. We will check against this state to determine if we need to reprocess the data. \n",
    "\n",
    "To do this, we will have to convert the processing steps into callable functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## im2rec Function Calls\n",
    "The following functions use the im2rec.py tool to process the image inputs.\n",
    "\n",
    "[https://mxnet.apache.org/api/faq/recordio](https://mxnet.apache.org/api/faq/recordio) (Accessed on 3/20/2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LST File Generation\n",
    "This function performs the first step, creating LST mapping files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2rec_lst(status, dataset):\n",
    "    lst_file_name = status + \"_subset_\" + dataset\n",
    "    lst_file_path = os.path.join(\"model_input_files\", lst_file_name)\n",
    "    folder_name = status + \"_subset\"\n",
    "    img_loc =  os.path.join(\"imgs\", dataset, folder_name)\n",
    "    \n",
    "    %run tools/im2rec.py {lst_file_path} {img_loc} --list --recursive \n",
    "    \n",
    "    return img_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recordio Conversion\n",
    "This function converts the images into a RECORDIO binary file format. In addition to converting images from raw image files to RECORDIO, we need to resize the images. In the Amazon Sagemaker Image Classification Hyperparameters documentation ([here](https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html)) there is a parameter called **image_shape**. This parameter requires a string with three numbers, comma seperated (i.e. \"1,2,3\").\n",
    "\n",
    "The first value is **num_channels**. This is the number of channels our input images have. They are color images, so they have three channels (Red, Green, and Blue aka RGB). The second and third values are the heigth and width of the images, in pixels. Technically, the algorithm can accomadate images of any size, but if the images are too large, there may be memory constraints. It indicates typical dimensions are 244 x 244. Our images come are 640 x 480. By area this is 5x bigger. So, we need to resize the images.\n",
    "\n",
    "The tool **im2rec.py** is capable of resizing images during the RECORDIO conversion. All we need to to is add the argument \"**--resize #**\" to the command line entry. The number in the command line argument is what the tool will resize the ***shortest edge*** of the input image. The larger edge will be resized to maintain the same relative dimensions. \n",
    "\n",
    "Therefore, we are going to resize the input images so that the resulting area is approximately equal to the area of a 244 x 244 image. The dimensions we are going to use is 210 x 280. This maintains the relative size of the input images, while have an area 98.7% that of the default image size. So, in the command line argument, the call will be **--resize 210**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2rec_rec(status, dataset, img_loc):\n",
    "    lst_file_name_ext = status + \"_subset_\" + dataset +\".lst\"\n",
    "    lst_file_path = os.path.join(\"model_input_files\", lst_file_name_ext)\n",
    "    \n",
    "    %run tools/im2rec.py {lst_file_path} {img_loc} --resize 210"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Hash Checksum\n",
    "The following function generates the Hash of a directory (training or validation images) and writes the Hash in a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_hash(status,file_name, file_path, file_dir):\n",
    "\n",
    "    # Generate Hash\n",
    "    dir_hash = dirhash(file_dir, 'sha256')\n",
    "    \n",
    "    # Write to CSV\n",
    "    dict_dir_hash = {'hash': [dir_hash]}\n",
    "    df_dir_hash = pd.DataFrame(dict_dir_hash)\n",
    "    df_dir_hash.to_csv(file_path, index=False)\n",
    "    print(\"Hash for {} images successfully written to CSV.\".format(status))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Processing and Writing Hash\n",
    "Whenever we process the input data we need to write a new hash, and vice versa. Therefore, for simplicity in the final code, we will lump these actions together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2rec_and_write(status, subfolder, file_name, file_path, file_dir):\n",
    "        print(\"Generating LST File: {}\".format(status))\n",
    "        \n",
    "        img_loc = im2rec_lst(status, subfolder)\n",
    "        \n",
    "        print(\"Starting generating REC file: {}\".format(status))\n",
    "        \n",
    "        im2rec_rec(status, subfolder, img_loc)\n",
    "        \n",
    "        print(\"REC File complete.\")\n",
    "        \n",
    "        write_hash(status,file_name, file_path, file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Flow Function\n",
    "The following function will verify the hash of the current files matches the older record. If they match, no action is taken. If they do not match, the input data is processed, and a new hash is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_hash(status, dataset):\n",
    "    \n",
    "    # Verify dataset variable is correct.\n",
    "    dataset_list = [\"sample\", \"complete\"]\n",
    "    \n",
    "    if dataset not in dataset_list:\n",
    "        print(\"Error. Correct Dataset Type Not Entered.\")\n",
    "        print(\"Dataset must be either type sample or complete.\")\n",
    "        return\n",
    "    \n",
    "    print()\n",
    "    print(\"Dataset: {},  Input Class: {}\".format(dataset, status))\n",
    "    print()\n",
    "            \n",
    "              \n",
    "    # Generate File Name, Path and Directory\n",
    "    file_name = \"imgs_{}_{}_subset_hash.csv\".format(dataset,status)\n",
    "    file_path = os.path.join(\"hash\", file_name)\n",
    "    folder_name = status + \"_subset\"\n",
    "    file_dir = os.path.join('imgs', dataset, folder_name)\n",
    "    \n",
    "    # Print for Sanity Check\n",
    "    print(\"File Name: {}\".format(file_name))\n",
    "    print(\"File Path: {}\".format(file_path))\n",
    "    print(\"File Directory: {}\".format(file_dir))\n",
    "    print()\n",
    "    \n",
    "    # Generate Current Hash\n",
    "    hash_new = dirhash(file_dir, 'sha256')\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"Hash for {} {} images do not exist.\".format(dataset, status))\n",
    "        print()\n",
    "        im2rec_and_write(status, dataset, file_name, file_path, file_dir)\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        # Read Existing Hash\n",
    "        df_hash_old = pd.read_csv(file_path)\n",
    "        hash_old = df_hash_old.iloc[0]['hash']\n",
    "        \n",
    "        if hash_old == hash_new:\n",
    "            print(\"Hash for {} images are equal.\".format(status))\n",
    "            print(\"New Hash not generated. Processing not required.\")\n",
    "            print()\n",
    "            return\n",
    "        \n",
    "        else:\n",
    "            print(\"Hash for {} images are NOT equal.\".format(status))\n",
    "            print()\n",
    "            im2rec_and_write_complete(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Process\n",
    "Runnign the following code chuck will execute the process flow function develpoed above for both training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: complete,  Input Class: train\n",
      "\n",
      "File Name: imgs_complete_train_subset_hash.csv\n",
      "File Path: hash\\imgs_complete_train_subset_hash.csv\n",
      "File Directory: imgs\\complete\\train_subset\n",
      "\n",
      "Hash for train images are equal.\n",
      "New Hash not generated. Processing not required.\n",
      "\n",
      "\n",
      "Dataset: complete,  Input Class: validation\n",
      "\n",
      "File Name: imgs_complete_validation_subset_hash.csv\n",
      "File Path: hash\\imgs_complete_validation_subset_hash.csv\n",
      "File Directory: imgs\\complete\\validation_subset\n",
      "\n",
      "Hash for validation images are equal.\n",
      "New Hash not generated. Processing not required.\n",
      "\n",
      "\n",
      "Dataset: sample,  Input Class: train\n",
      "\n",
      "File Name: imgs_sample_train_subset_hash.csv\n",
      "File Path: hash\\imgs_sample_train_subset_hash.csv\n",
      "File Directory: imgs\\sample\\train_subset\n",
      "\n",
      "Hash for train images are equal.\n",
      "New Hash not generated. Processing not required.\n",
      "\n",
      "\n",
      "Dataset: sample,  Input Class: validation\n",
      "\n",
      "File Name: imgs_sample_validation_subset_hash.csv\n",
      "File Path: hash\\imgs_sample_validation_subset_hash.csv\n",
      "File Directory: imgs\\sample\\validation_subset\n",
      "\n",
      "Hash for validation images are equal.\n",
      "New Hash not generated. Processing not required.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status_list = [\"train\", \"validation\"]\n",
    "dataset_list = [\"complete\", \"sample\"]\n",
    "\n",
    "for dataset in dataset_list:\n",
    "    for status in status_list:\n",
    "        verify_hash(status, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish AWS Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = 'dsba_6190_proj_4'\n",
    "region = 'us-east-1'\n",
    "bucket = 'dsba-6190-final-team-project'\n",
    "prefix = \"channels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session(profile_name = profile,\n",
    "                               region_name = region)\n",
    "\n",
    "s3_resource = session.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload\n",
    "We now upload the Recordio format files to S3. The Amazon Sagemaker Algorithm requires a specific folder format, with the training and validation data as seperate subfolders of the same directory, with the names train and validation, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(status, dataset):\n",
    "    # Establish Dynamic File and Path Names\n",
    "    folder = \"model_input_files\"\n",
    "    file_name_local = status + \"_subset_\"  + dataset + \".rec\"\n",
    "    file_name_s3 = status +  \".rec\"\n",
    "    \n",
    "    # Define Boto3 Resource Inputs\n",
    "    file_for_upload =  os.path.join(folder, file_name_local).replace('\\\\','/')\n",
    "    s3_key = os.path.join(prefix, dataset, status, file_name_s3).replace('\\\\','/')\n",
    "    \n",
    "    print(\"File for Upload:\\t{}\".format(file_for_upload))\n",
    "    print(\"s3 Key:\\t\\t\\t{}\".format(s3_key))\n",
    "    \n",
    "    # Upload\n",
    "    s3_resource.Bucket(bucket).upload_file(Filename = file_for_upload, Key = s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for Upload:\tmodel_input_files/train_subset_complete.rec\n",
      "s3 Key:\t\t\tchannels/complete/train/train.rec\n"
     ]
    }
   ],
   "source": [
    "status = status_list[0]\n",
    "dataset = dataset_list[0]\n",
    "\n",
    "#upload_to_s3(status, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for Upload:\tmodel_input_files/validation_subset_complete.rec\n",
      "s3 Key:\t\t\tchannels/complete/validation/validation.rec\n"
     ]
    }
   ],
   "source": [
    "status = status_list[1]\n",
    "dataset = dataset_list[0]\n",
    "\n",
    "#upload_to_s3(status, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for Upload:\tmodel_input_files/train_subset_sample.rec\n",
      "s3 Key:\t\t\tchannels/sample/train/train.rec\n"
     ]
    }
   ],
   "source": [
    "status = status_list[0]\n",
    "dataset = dataset_list[1]\n",
    "\n",
    "#upload_to_s3(status, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for Upload:\tmodel_input_files/validation_subset_sample.rec\n",
      "s3 Key:\t\t\tchannels/sample/validation/validation.rec\n"
     ]
    }
   ],
   "source": [
    "status = status_list[1]\n",
    "dataset = dataset_list[1]\n",
    "\n",
    "#upload_to_s3(status, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Checksum in S3\n",
    "In order to not accidentally upload duplicate files to S3, I want to cross-check the Checksum of the local file and the S3 file. The following functions approximate that. Note that technically the ETAG in S3 is not always the MD5. But our upload process is simple, and therefore the ETAG should equate to MD5 Checksum. Do not use this method if performing batch uploads.\n",
    "\n",
    "The following functions were found on the following blog post:\n",
    "\n",
    "[https://zihao.me/post/calculating-etag-for-aws-s3-objects/](https://zihao.me/post/calculating-etag-for-aws-s3-objects/) (Accessed on 3/21/2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def md5_checksum(filename):\n",
    "    m = hashlib.md5()\n",
    "    with open(filename, 'rb') as f:\n",
    "        for data in iter(lambda: f.read(1024 * 1024), b''):\n",
    "            m.update(data)\n",
    "    return m.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etag_checksum(filename, chunk_size=8 * 1024 * 1024):\n",
    "    md5s = []\n",
    "    with open(filename, 'rb') as f:\n",
    "        for data in iter(lambda: f.read(chunk_size), b''):\n",
    "            md5s.append(hashlib.md5(data).digest())\n",
    "    m = hashlib.md5(\"\".join(md5s))\n",
    "    return '{}-{}'.format(m.hexdigest(), len(md5s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etag_compare(filename, etag):\n",
    "    et = etag[1:-1] # strip quotes\n",
    "    if '-' in et and et == etag_checksum(filename):\n",
    "        return True\n",
    "    if '-' not in et and et == md5_checksum(filename):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for Upload:\tmodel_input_files/train_subset_complete.rec\n",
      "s3 Key:\t\t\tchannels/complete/train/train.rec\n",
      "Local File MD5 Checksum: 5cccc4891b86b3679d48508c9bc4705e\n",
      "\"666c6c08660a90ae1c58936372eb3f18-149\"\n"
     ]
    }
   ],
   "source": [
    "status = status_list[0]\n",
    "dataset = dataset_list[0]\n",
    "\n",
    "folder = \"model_input_files\"\n",
    "file_name_local = status + \"_subset_\"  + dataset + \".rec\"\n",
    "file_name_s3 = status +  \".rec\"\n",
    "\n",
    "file_for_upload =  os.path.join(folder, file_name_local).replace('\\\\','/')\n",
    "s3_key = os.path.join(prefix, dataset, status, file_name_s3).replace('\\\\','/')\n",
    "\n",
    "print(\"File for Upload:\\t{}\".format(file_for_upload))\n",
    "print(\"s3 Key:\\t\\t\\t{}\".format(s3_key))\n",
    "\n",
    "md5_local = md5_checksum(file_for_upload)\n",
    "\n",
    "print(\"Local File MD5 Checksum: {}\".format(md5_local))\n",
    "\n",
    "#S3\n",
    "obj = s3_resource.Object(bucket, s3_key)\n",
    "\n",
    "print(obj.e_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666c6c08660a90ae1c58936372eb3f18-149\n"
     ]
    }
   ],
   "source": [
    "obj = s3_resource.Object(bucket, s3_key)\n",
    "\n",
    "print(obj.e_tag[1:-1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "State Farm Distracted Driver Process Flow.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
